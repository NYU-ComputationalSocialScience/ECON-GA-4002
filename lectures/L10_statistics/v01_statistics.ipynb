{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Statistics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.stats as st\n",
    "\n",
    "from ipywidgets import interact, FloatSlider, IntSlider\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### What is a random variable?\n",
    "\n",
    "A random variable is a variable that takes a stochastic value from a set of possible outcomes.\n",
    "\n",
    "* Let $\\mathcal{Y}$ denote the set of all possible outcomes\n",
    "* Let $f_Y$ be the probability distribution, then for any subset $A \\in \\mathcal{Y}$, $f_Y(A) \\equiv \\text{Probability}(A) = \\int_{x \\in A} f_Y(x) dx$\n",
    "* We will use $Y \\in \\mathcal{Y}$ to denote a vector of random variables (potentially of length 1)\n",
    "* We will use $y$ to denote a particular realization of the random variable $Y$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "A sequence of i.i.d. random variables is itself a random variable.\n",
    "\n",
    "Let $\\{Y_i\\}$ be a sequence of independent and identically distributed random variables, then $\\forall i$ $Y_i \\sim f_Y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Example**:\n",
    "\n",
    "If $Y$ is a Normal random variable with mean $\\mu$ and standard deviation $\\sigma$, then the set of outcomes is $\\mathbb{R}$ and the probability density function of the probability distribution is given by\n",
    "\n",
    "$$f(x) = \\frac{1}{\\sigma \\sqrt{2 \\pi}} \\exp^{-\\frac{(x - \\mu)^2}{2 \\sigma^2}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "mu = 0.0\n",
    "sigma = 1.0\n",
    "\n",
    "Y = st.norm(mu, sigma)\n",
    "\n",
    "y = Y.rvs(size=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(Y)  # A random variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "Y.support()  # The possible outcomes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "lspace = np.linspace(-5.0, 5.0, 100)\n",
    "ax.plot(lspace, Y.pdf(lspace))  # Probability density function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "print(y)  # Observations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### A function of a random variable is a random variable\n",
    "\n",
    "Let $X$ be a random variable such that $X \\equiv g(Y)$ where $g$ is an invertible function.\n",
    "\n",
    "Let $z$ be a constant. $g(X)$ is a random variable with probability distribution given by:\n",
    "\n",
    "$$F_X(z) = \\text{Probability}(X \\leq z) = \\text{Probability}(g(Y) \\leq z) = F_Y(g^{-1}(z))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Example**\n",
    "\n",
    "Let $Y$ be a normally distributed random variable with mean 0 and standard deviation 1.\n",
    "\n",
    "If $X \\equiv \\exp(Y)$, what is the probability distribution associated with $X$?\n",
    "\n",
    "\\begin{align*}\n",
    "  \\text{Prob}(x \\leq z) &= \\text{Prob}(\\exp(y) \\leq z) \\\\\n",
    "  &= \\text{Prob}(y \\leq \\log(z)) \\\\\n",
    "\\end{align*}\n",
    "\n",
    "One can show that this produces the cumulative density function that corresponds to the log-normal distribution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "mu = 0.0\n",
    "sigma = 1.0\n",
    "\n",
    "Y = st.norm(loc=mu, scale=sigma)\n",
    "Z = st.lognorm(scale=np.exp(mu), s=sigma)  # Read docs on why parameterized this way\n",
    "\n",
    "x = np.exp(Y.rvs(100_000))\n",
    "z = Z.rvs(100_000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.hist(x, bins=50, alpha=0.25, color=\"red\", density=True)\n",
    "ax.hist(z, bins=50, alpha=0.25, color=\"black\", density=True)\n",
    "\n",
    "ax.set_xlim(0.0, 25.0);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### What is a statistic??\n",
    "\n",
    "Let $Y$ be a random variable. Let $\\tilde{y}$ be an i.i.d. sample of length $n$ from the $Y$.\n",
    "\n",
    "A statistic is a function, $g$, that maps samples from $Y$, denoted by $\\tilde{y}$, into a scalar or vector.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This means... A statistic is a random variable because it is a function of a random variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Example: Sample mean**\n",
    "\n",
    "A sample mean is a statistic.\n",
    "\n",
    "$$\\bar{y} \\equiv \\frac{1}{n} \\sum_i \\tilde{y}_i$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "Y = st.norm(0.0, 1.0)\n",
    "\n",
    "ytilde = Y.rvs(10_000)\n",
    "\n",
    "np.mean(ytilde)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Example: Counting**\n",
    "\n",
    "Let $Y$ be a discrete random variable with $m$ possible values denoted $\\{\\bar{y}_i\\}_{i=1}^m$. Counting the number of occurrences of each of the $m$ possible values is a statistic.\n",
    "\n",
    "$$\\{c_{i}\\}_{i=1}^m \\equiv \\left\\{ \\sum_{j=1}^n \\mathbb{1}_{\\tilde{y}_j == \\bar{y}_i}\\right\\}_{i=1}^m$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Random variable that takes integers from lb to ub-1\n",
    "# with equal probability\n",
    "Y = st.randint(0, 5)\n",
    "\n",
    "ytilde = Y.rvs(5_000)\n",
    "\n",
    "np.array([np.sum(ytilde == i) for i in range(0, 5)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### What is a model?\n",
    "\n",
    "A model is a probability distribution for $Y$ indexed by a vector of parameters ($\\theta$)\n",
    "\n",
    "We write this as $f(Y | \\theta)$ or $f_{\\theta}(Y)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Example**\n",
    "\n",
    "Suppose that quarterly GDP growth can be described by\n",
    "\n",
    "$$y = \\theta + \\varepsilon$$\n",
    "\n",
    "where $\\theta = 1.5$ and $\\varepsilon \\sim N(0, 1)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def plot_model_distribution(theta):\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    # Create  grids on the x and y plane\n",
    "    yvals = np.linspace(-10.0, 10.0, 200)\n",
    "\n",
    "    # Parameters\n",
    "    f_epsilon = st.norm(0.0, 1.0)\n",
    "\n",
    "    # Create\n",
    "    prob = f_epsilon.pdf(yvals - theta)\n",
    "\n",
    "    ax.plot(yvals, prob)\n",
    "\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "    ax.set_xlabel(r\"$y$\")\n",
    "    ax.set_ylabel(r\"$f(y)$\")\n",
    "    ax.set_title(\"A single model\");\n",
    "\n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "plot_model_distribution(-2.5);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Manifolds of models\n",
    "\n",
    "A manifold of statistical models consists of a set of joint probability distributions, $f(Y | \\theta)$, \"swept out\" by $\\theta \\in \\Theta$.\n",
    "\n",
    "Once we pick a $\\theta$, we have a model and can simulate or do anything else that we might want do with a model.\n",
    "\n",
    "(_spoiler alert: we'll talk about \"how to pick a $\\theta$\" soon_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Example**\n",
    "\n",
    "A manifold of models for quarterly GDP growth could be described by\n",
    "\n",
    "$$y = \\theta + \\varepsilon$$\n",
    "\n",
    "for $\\theta \\in \\Theta$ and where $\\varepsilon \\sim N(0, 1)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def plot_manifolds(theta=1.5):\n",
    "\n",
    "    # Values for plotting\n",
    "    thetas = np.linspace(-3.5, 3.5, 50)\n",
    "    yvals = np.linspace(-10.0, 10.0, 500)\n",
    "    tt, yy = np.meshgrid(thetas, yvals)\n",
    "    mod_probs = st.norm(0.0, 1.0).pdf(yvals - theta)\n",
    "    mf_probs = st.norm(0.0, 1.0).pdf(yy - tt)\n",
    "\n",
    "    # Create figure\n",
    "    fig = plt.figure(figsize=(10, 8))\n",
    "    fig.suptitle(\"Manifold of Models\")\n",
    "\n",
    "    ax1 = fig.add_subplot(projection=\"3d\")\n",
    "\n",
    "    ax1.plot_surface(\n",
    "        tt, yy, mf_probs,\n",
    "        cmap=\"viridis\", linewidth=0.0, alpha=0.5,\n",
    "        cstride=2, rstride=2\n",
    "    )\n",
    "    ax1.plot(yvals, mod_probs, zs=-5.0, zdir=\"x\")\n",
    "    ax1.plot(\n",
    "        yvals, mod_probs, zs=theta, zdir=\"x\",\n",
    "        color=\"black\", linewidth=2.0\n",
    "    )\n",
    "\n",
    "    ax1.set_xlabel(r\"$\\theta$\", size=10)\n",
    "    ax1.set_ylabel(r\"$y$\", size=10)\n",
    "    ax1.set_zlabel(r\"$f(y)$\", size=10)\n",
    "    ax1.set_xlim(-5.0, 4.0)\n",
    "    ax1.set_ylim(-12.0, 12.0)\n",
    "    ax1.view_init(25, -20)\n",
    "\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "theta_slider = FloatSlider(\n",
    "    value=1.5, min=-3.5, max=3.5, step=0.5,\n",
    "    description=\"Value of theta\"\n",
    ")\n",
    "\n",
    "interact(plot_manifolds, theta=theta_slider);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Likelihood function\n",
    "\n",
    "Imagine you have a manifold of models indexed by $\\theta$ and someone asks you how likely a particular group of observations are to have come from different elements of your manifold.\n",
    "\n",
    "How could you answer this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The **likelihood function** for a manifold of statistical models is a function that maps values of the parameters, $\\theta \\in \\Theta$ and observations, $\\tilde{y} \\in \\mathcal{Y}$, into the probability of having observed $\\tilde{y}$ given $\\theta$.\n",
    "\n",
    "$$\\mathcal{L}(\\theta | \\tilde{y}) \\equiv f(\\tilde{y} | \\theta)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Example**\n",
    "\n",
    "Let's continue to build on our quarterly GDP growth example. The manifold of models is given by:\n",
    "\n",
    "$$y = \\theta + \\varepsilon$$\n",
    "\n",
    "for $\\theta \\in \\Theta$ and where $\\varepsilon \\sim N(0, 1)$.\n",
    "\n",
    "Let $\\tilde{y}$ be $n$ i.i.d. observations generated by a model from the manifold of models above.\n",
    "\n",
    "What's the likelihood that this sequence was generated by a particular model indexed by $\\hat{\\theta} \\in \\Theta$?\n",
    "\n",
    "\\begin{align*}\n",
    "  \\mathcal{L}(\\hat{\\theta} | \\tilde{y}) &= f(\\tilde{y} | \\hat{\\theta}) \\\\\n",
    "  &= f(\\tilde{y}_1 | \\hat{\\theta}) f(\\tilde{y}_2 | \\hat{\\theta}) \\dots f(\\tilde{y}_n) \\\\\n",
    "  &= \\prod_{i=1}^n \\phi(\\tilde{y}_i - \\hat{\\theta}))\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def log_likelihood(theta_hat, ytilde):\n",
    "    \"\"\"\n",
    "    Computes the log likelihood of observing a particular\n",
    "    group of observations\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    theta_hat : float\n",
    "        The model parameter\n",
    "    ytilde : np.array(float, ndim=1)\n",
    "        i.i.d. sequences of observations\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    log_ll : float\n",
    "        The log of the likelihood function -- This is\n",
    "        more numerically stable than the raw likelihood\n",
    "    \"\"\"\n",
    "    # Compute likelihood of each individual observation\n",
    "    ll = st.norm(0.0, 1.0).pdf(ytilde - theta_hat)\n",
    "\n",
    "    # Take logs and sum rather than take product\n",
    "    log_ll = np.sum(np.log(ll))\n",
    "    \n",
    "    return log_ll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "log_likelihood(0.0, np.random.randn(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Below we take observations of quarterly GDP growth from the United States between the years of 1990 and 2015."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# US quarterly GDP growth from 1990 to 2015\n",
    "ytilde = np.array([\n",
    "    2.18, 1.49, 0.92, -0.17, 0.51, 1.52, 1.29, 0.94, 1.57, 1.69, 1.48, 1.74, 0.73, 1.18, 1.07, 1.91,\n",
    "    1.45, 1.84, 1.16, 1.69, 0.9, 0.78, 1.35, 1.16, 1.23, 2.09, 1.23, 1.58, 1.25, 1.87, 1.69, 1.19, 1.15,\n",
    "    1.16, 1.69, 1.9, 1.33, 1.14, 1.66, 2.25, 1.05, 2.45, 0.7, 1.16, 0.32, 1.19, -0.01, 0.6, 1.21, 0.97,\n",
    "    0.91, 0.72, 1.01, 1.16, 2.25, 1.75, 1.28, 1.58, 1.61, 1.78, 1.91, 1.17, 1.8, 1.44, 2.04, 1.07, 0.86,\n",
    "    1.22, 1.22, 1.22, 1.06, 1.01, -0.21, 1.06, 0.2, -1.86, -1.13, -0.29, 0.47, 1.44, 0.64, 1.39, 1.03, 1.07,\n",
    "    0.3, 1.38, 0.62, 1.31, 1.41, 0.83, 0.65, 0.63, 1.29, 0.41, 1.27, 1.39, 0.13, 1.92, 1.66, 0.72, 0.86,\n",
    "    1.22, 0.68, 0.17\n",
    "])\n",
    "\n",
    "for _th in [0.0, 1.0, 2.0]:\n",
    "    print(f\"Log likelihood of being generated by model indexed by theta={_th} is\")\n",
    "    print(\"\\t\", log_likelihood(_th, ytilde))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def plot_likelihood(ytilde):\n",
    "    # Values for plotting\n",
    "    thetas = np.linspace(-3.5, 3.5, 50)\n",
    "    log_ll = np.array([log_likelihood(_th, ytilde) for _th in thetas])\n",
    "\n",
    "    # Create figure\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    fig.suptitle(\"Likelihood function\", size=16)\n",
    "\n",
    "    ax.plot(thetas, log_ll)\n",
    "\n",
    "    ax.set_xlabel(r\"$\\theta$\", size=12)\n",
    "    ax.set_ylabel(r\"$\\log \\mathcal{L}(\\theta | \\tilde{y})$\", size=12)\n",
    "\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "\n",
    "    return fig, ax\n",
    "\n",
    "plot_likelihood(ytilde);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Direct and Inverse Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Direct problem\n",
    "\n",
    "The _direct problem_ is to draw a sample, $\\tilde{y}$, from a given model. We also call this \"simulating\" or \"sampling from the model\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Input to the direct problem**: $\\theta$\n",
    "\n",
    "**Output of the direct problem**: $\\tilde{y}$\n",
    "\n",
    "Once you have samples from the model, $\\tilde{y}$, the samples can be used to generate statistics associated with the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Inverse problem\n",
    "\n",
    "The inverse problem takes a given manifold of models and a sample generated from one of these models, $\\tilde{y}$, and infers which model from the manifold generated the data. This is also sometimes called \"statistical inference\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Input to the inverse problem**: $\\tilde{y}$, $\\{f(\\tilde{y} | \\theta) \\; \\forall \\theta \\in \\Theta\\}$\n",
    "\n",
    "**Output of the inverse problem**: $\\hat{\\theta}$\n",
    "\n",
    "Once you have chosen a $\\hat{\\theta} \\in \\Theta$, you have a model that you could use in the direct problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Let $\\tilde{y}$ be a sample of $n$ i.i.d observations generated by a model from a particular manifold of models.\n",
    "\n",
    "An estimator is a function that maps a sample, $\\tilde{y}$, into a model from the manifold of models. We will use $\\hat{\\theta}_n$ (or just $\\hat{\\theta}$) to be the estimator of $\\theta$ for a given manifold of models.\n",
    "\n",
    "Estimators of parameters are statistics, but statistics are not necessarily estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Advisory!**\n",
    "\n",
    "The parameter $\\theta$ of a model is fixed. It is **not** a random variable.\n",
    "\n",
    "However, $\\hat{\\theta}_n$ is a statistic which means it **is** a random variable, which means that it has a sampling distribution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Characteristics of estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "**Consistent**\n",
    "\n",
    "Given $\\tilde{y}$ (of length $n$), an estimator, $\\hat{\\theta}_n$, of a parameter, $\\theta$, is consistent if\n",
    "\n",
    "$$\\hat{\\theta}_n \\overset{p}{\\to} \\theta$$\n",
    "\n",
    "This definition should remind you of the law of large numbers! The law or large numbers is going to play a central role in checking the consistency of an estimator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Example 1**\n",
    "\n",
    "Consider our manifold of models given by\n",
    "\n",
    "$$y = \\theta + \\varepsilon$$\n",
    "\n",
    "for $\\theta \\in \\Theta$ and where $\\varepsilon \\sim N(0, 1)$.\n",
    "\n",
    "Let $\\tilde{y}$ be a sample of $n$ i.i.d. observations. Consider an estimator of $\\theta$, $g^{\\dagger}$, be defined by:\n",
    "\n",
    "$$g^{\\dagger}(\\tilde{y}) = \\frac{1}{n} \\sum_i \\tilde{y}_i$$\n",
    "\n",
    "Is $\\theta^{\\dagger} \\equiv g^{\\dagger}(\\tilde{y})$ consistent?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The estimator is defined by:\n",
    "\n",
    "\\begin{align*}\n",
    "  g^{\\dagger}(\\tilde{y}) &= \\frac{1}{n} \\sum_i \\tilde{y}_i \\\\\n",
    "  &= \\frac{1}{n} \\sum_i \\theta + \\varepsilon_i \\\\\n",
    "  &= \\theta + \\frac{1}{n} \\sum_i \\varepsilon_i \\\\\n",
    "\\end{align*}\n",
    "\n",
    "The LLN tells us $\\frac{1}{n} \\sum_i \\varepsilon_i \\overset{p}{\\to} 0$ which means $g^{\\dagger}(\\tilde{y}) \\overset{p}{\\to} \\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "... This case was a little easy. What happens when we have more complicated models?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Maximum Likelihood\n",
    "\n",
    "We now formally introduce a class of estimators we hinted at earlier known as the \"maximum likelihood\" estimators.\n",
    "\n",
    "These estimators are defined by finding $\\hat{\\theta}$ such that $\\mathcal{L}(\\hat{\\theta} | \\tilde{y}) = \\max_{\\theta} f(\\tilde{y} | \\theta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Properties of maximum likelihood\n",
    "\n",
    "As long as certain conditions are satisfied,\n",
    "\n",
    "* Consistent\n",
    "* Sample distribution of $\\hat{\\theta}$ is approximately normal\n",
    "* Efficient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Computing the maximum likelihood estimate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Example**\n",
    "\n",
    "Consider our quarterly GDP growth example. The manifold of models is given by:\n",
    "\n",
    "$$y = \\theta + \\varepsilon$$\n",
    "\n",
    "for $\\theta \\in \\Theta$ and where $\\varepsilon \\sim N(0, 1)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Recall that our likelihood function was given by\n",
    "\n",
    "\\begin{align*}\n",
    "  \\mathcal{L}(\\hat{\\theta} | \\tilde{y}) &= \\prod_{i=1}^n \\phi(\\tilde{y}_i - \\hat{\\theta})) \\\\\n",
    "  &= \\prod_{i=1}^n \\frac{1}{\\sigma \\sqrt{2 \\pi}} \\exp \\left(\\frac{(\\tilde{y}_i - \\theta)^2}{2 \\sigma^2}\\right) \\\\\n",
    "  &= \\frac{1}{\\sigma \\sqrt{2 \\pi}} \\exp \\left(\\sum_i \\frac{(\\tilde{y}_i - \\theta)^2}{2 \\sigma^2}\\right)\n",
    "\\end{align*}\n",
    "\n",
    "We can maximize by taking derivatives with respect to $\\theta$ and setting equal to 0,\n",
    "\n",
    "\\begin{align*}\n",
    "  \\frac{\\partial \\mathcal{L}(\\hat{\\theta} | \\tilde{y})}{\\partial \\theta} &= \\frac{1}{\\sigma \\sqrt{2 \\pi}} \\exp \\left(\\sum_i \\frac{(\\tilde{y}_i - \\theta)^2}{2 \\sigma^2}\\right) \\left(\\sum_i \\frac{-(\\tilde{y}_i - \\theta)}{ \\sigma^2}\\right) = 0 \\\\\n",
    "\\end{align*}\n",
    "\n",
    "In order to set this to 0, we need the product of the 3 pieces to be 0... Only the last component could be equal to 0, so\n",
    "\n",
    "\\begin{align*}\n",
    "  0 &= \\left(\\sum_i \\frac{-(\\tilde{y}_i - \\theta)}{ \\sigma^2}\\right) \\\\\n",
    "  &\\rightarrow \\theta = \\frac{1}{n} \\sum_i \\tilde{y}_i\n",
    "\\end{align*}\n",
    "\n",
    "^Notice that we chose a particular estimator for the previous section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We already have a function that can evaluate the log-likelihood, so let's check our answer numerically!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import scipy.optimize as opt\n",
    "\n",
    "f = lambda x: -log_likelihood(x, ytilde)\n",
    "\n",
    "sol = opt.minimize(f, x0=np.array([2.0]))\n",
    "\n",
    "print(f\"Optimizer says: {sol.x[0]}\")\n",
    "print(f\"Sample mean is: {np.mean(ytilde)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Identification\n",
    "\n",
    "Identification has a precise definition.\n",
    "\n",
    "A parameter vector, $\\theta \\in \\Theta$ is said to be _identified_ by observations $\\tilde{y} \\in \\mathcal{y}$ if the maximum likelihood estimate of $\\theta$ is unique.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "plot_likelihood(ytilde);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "When might a model not be identified?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "A model is not identified when there is no unique maximum to the maximum likelihood function. This can arise because"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "1. The parameters that index the models in your manifold are hard (impossible) to learn about\n",
    "  - Consider parameters $\\beta_1, \\beta_2$ that index a manifold of models described by $y = (\\beta_1 + \\beta_2) x + \\varepsilon$.\n",
    "  - Cannot separate $\\beta_1$ or $\\beta_2$ in that model no matter how many values of $y$ you are given"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "beta = np.linspace(-2.0, 2.0, 200)\n",
    "beta_1, beta_2 = np.meshgrid(beta, beta)\n",
    "foo = st.norm(0, 0.25).pdf(beta_1 - beta_2)\n",
    "\n",
    "ax.set_xlabel(r\"$\\beta_1$\")\n",
    "ax.set_ylabel(r\"$\\beta_2$\")\n",
    "ax.pcolor(\n",
    "    beta_1, beta_2, foo,\n",
    "    shading=\"nearest\"\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "2. The data provides insufficient information\n",
    "  - Consider parameters $\\beta_1$ and $\\beta_2$ that index a manifold of models described by $y = \\beta_1 x_1 + \\beta_2 x_2 + \\varepsilon$ with $\\tilde{y} = \\begin{bmatrix} 0.5 \\end{bmatrix}$, $x_1 = \\begin{bmatrix} 1.0 \\end{bmatrix}$, $x_2 = \\begin{bmatrix} 1.5 \\end{bmatrix}$\n",
    "  - Will only be able to determine that $0.5 = \\beta_1 + 1.5 \\beta_2$\n",
    "  - This is similar to previous example in the sense that it will create a parameter ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Another Example:\n",
    "\n",
    "For the sake of variety, we now move on to a slightly different class of model than we have been using."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Consider a class of 50 students at a university.\n",
    "\n",
    "Let each student's \"innate\" ability be observable and given by a random variable, $A \\sim N(0, 1)$.\n",
    "\n",
    "Each student passes their classes with a probability\n",
    "\n",
    "$$p_i = \\frac{1}{1 + \\gamma_1 e^{-\\gamma_2 a_i}}$$\n",
    "\n",
    "where $a_i$ is their ability level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def compute_p(g1, g2, a):\n",
    "    return 1 / (1 + g1*np.exp(-g2*a))\n",
    "\n",
    "def plot_p(g1, g2):\n",
    "    a = np.linspace(-3.0, 3.0, 50)\n",
    "    p = compute_p(g1, g2, a)\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "    ax.plot(a, p)\n",
    "    \n",
    "    return None\n",
    "    \n",
    "g1_slider = FloatSlider(\n",
    "    value=0.5, min=0.00, max=3.5, step=0.1,\n",
    "    description=\"Value of g1\"\n",
    ")\n",
    "g2_slider = FloatSlider(\n",
    "    value=0.3, min=0.00, max=3.5, step=0.1,\n",
    "    description=\"Value of g2\"\n",
    ")\n",
    "\n",
    "interact(plot_p, g1=g1_slider, g2=g2_slider);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Suppose that $\\gamma_1 = 0.3$, and $\\gamma_2 = 1.5$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Direct problem**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "g1 = 0.3\n",
    "g2 = 1.5\n",
    "n = 50\n",
    "\n",
    "# Generate abilities, passing probabilities\n",
    "a_i = np.random.randn(n)\n",
    "p_i = compute_p(g1, g2, a_i)\n",
    "\n",
    "# Use passing probabilities to determine whether students passed\n",
    "passed = np.random.rand(n) < p_i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "_Statistics of interest_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Fraction that passed is: {np.mean(passed)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Correlation between ability and pass is: {np.corrcoef(a_i, passed)[0, 1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Inverse problem**\n",
    "\n",
    "Given a manifold of models and a sample of data, can we infer which model generated our data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Likelihood**\n",
    "\n",
    "\\begin{align*}\n",
    "  \\mathcal{L}(\\theta | \\tilde{y}) &= f(\\tilde{y} | \\theta) \\\\\n",
    "  &= \\prod_i f(\\tilde{y}_i | \\theta) \\\\\n",
    "  &= \\prod_i p_i \\mathbb{1}_{\\text{pass}} + (1 - p_i) (1 - \\mathbb{1}_{\\text{pass}}) \\\\\n",
    "  &= \\prod_i \\frac{1}{1 + \\gamma_1 e^{-\\gamma_2 a_i}} \\mathbb{1}_{\\text{pass}} + (1 - \\frac{1}{1 + \\gamma_1 e^{-\\gamma_2 a_i}}) (1 - \\mathbb{1}_{\\text{pass}})\n",
    "\\end{align*}\n",
    "\n",
    "It's possible that there's a way to maximize this analytically, but we have great numerical tools at our disposal so we're going to pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def loglikelihood_uni(theta, a_i, ytilde):\n",
    "    # Unpack params and compute p_is\n",
    "    g1, g2 = theta\n",
    "    p_i = compute_p(g1, g2, a_i)\n",
    "\n",
    "    # Compute likelihoods\n",
    "    lls = np.array(\n",
    "        [p_i if ytilde[i] else (1 - p_i) for (i, p_i) in enumerate(p_i)]\n",
    "    )\n",
    "\n",
    "    return np.sum(np.log(lls))\n",
    "\n",
    "\n",
    "def neg_loglikelihood_uni(theta, a_i, ytilde):\n",
    "    return -loglikelihood_uni(theta, a_i, ytilde)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "loglikelihood_uni(np.array([0.2, 1.2]), a_i, passed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Moment of truth**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "sol = opt.minimize(\n",
    "    neg_loglikelihood_uni, np.array([1.75, 1.75]),\n",
    "    args=(a_i, passed), method=\"nelder-mead\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Parameters that generated the data were: {g1, g2}\")\n",
    "print(f\"Maximum likelihood parameters were: {sol.x}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### When to use maximum likelihood\n",
    "\n",
    "If you have a model or manifold of models, always write down the likelihood function. Fisher showed that if do maximum likelihood that the estimator has great asymptotic properties and is well behaved (everything a frequentist would want from an estimator!).\n",
    "\n",
    "If you can write it down..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "There are some models (economic models especially) where you can't write the likelihood function, however, you can often still simulate the model.\n",
    "\n",
    "If we can't write down a likelihood (but we can simulate it) then we are forced to turn to other methods which we will discuss next time."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
