{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More Linear Algebra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.linalg\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Special types of matrices\n",
    "\n",
    "Some matrices have special properties. Let's take a look at a few of them.\n",
    "\n",
    "## Toeplitz matrices\n",
    "\n",
    "A **Toeplitz matrix** A is an m\\*n matrix where all of the entries on a diagonal (from top left to bottom right) are the same. Here's what a 3\\*2 Toeplitz matrix looks like:\n",
    "\n",
    "$$ \\begin{bmatrix}\n",
    "a & b \\\\\n",
    "c & a \\\\\n",
    "d & c\n",
    "\\end{bmatrix} $$\n",
    "\n",
    "We can generate these using scipy as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = scipy.linalg.toeplitz([1,2,3], [1,4,5,6]) # The first entry is the first column of A and the second is the first row.\n",
    "                                              # A(1,1) is determined by the first list, so passing in (2,4,5,6) here wouldn't\n",
    "                                              # change the output.\n",
    "A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Properties\n",
    "\n",
    "- Toeplitz matrices are a subspace of m\\*n matrices.\n",
    "\n",
    "A Toeplitz matrix is much less computationally intensive than a generic matrix. The following operations can be performed in O(n<sup>2</sup>) time if A and B are n\\*n Toeplitz matrices:\n",
    "\n",
    "- Solving the system Ax = b (in scipy, we use solve_toeplitz, see below)\n",
    "- Computing the determinant of A\n",
    "- Computing the LU-decomposition of A (see below)\n",
    "- Multiplying A and B (provided their dimensions are compatible)\n",
    "\n",
    "In addition, A and B can be added in O(n) time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.linalg.solve_toeplitz(([1,2,3], [1,4,5]), [1, 1, 1]) # The first two vectors are the 1st column and 1st row of A. \n",
    "                                                           # The third vector is b."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = np.array([[1, .5, -.4],[.3, 1, -.9],[0.0, -.5, 1]]) # arbitrary noise weighting matrix\n",
    "\n",
    "V = C@np.transpose(C)  # associated covariance matrix\n",
    "print('V= ', V)\n",
    "v,lam = np.linalg.eig(V)\n",
    "print('v = ',v)\n",
    "print('lam = ', lam)\n",
    "c1 = V[:,0]\n",
    "r1 = V[0,:]\n",
    "print('c1 = ', c1)\n",
    "print('r1 = ', r1)\n",
    "Vt= scipy.linalg.toeplitz(np.transpose(c1),r1)\n",
    "print('V - Vt = ', V - Vt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Circulant matrices\n",
    "\n",
    "A **circulant matrix** is a special type of square Toeplitz matrix, which is obtained as follows:\n",
    "\n",
    "1. Start with the first row of our matrix, A. The entries can be any number.\n",
    "2. Let's take n = length(first row of A) = matrix size.\n",
    "3. Let's take k = 2.\n",
    "4. To find the k<sup>th</sup> row, take row (k-1) and shift all the entries one spot to the right. The first element of row k should be the last element of row k-1.\n",
    "5. Increment k by 1 unless k = n.\n",
    "\n",
    "Let's look at this process for a 3\\*3 matrix. We'll start with [1, 3, 5] as our first row.\n",
    "\n",
    "$$ [1, 3, 5] \\implies [3, 5, 1] \\implies [5, 1, 3] $$\n",
    "\n",
    "$$ A = \\begin{bmatrix}\n",
    "1 & 3 & 5 \\\\\n",
    "3 & 5 & 1 \\\\\n",
    "5 & 1 & 3\n",
    "\\end{bmatrix} $$\n",
    "\n",
    "We can also do this using scipy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = scipy.linalg.circulant([1, 2, 3]) # The parameter we pass to this function is the first row of A\n",
    "A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Properties\n",
    "\n",
    "Circulant matrices have special properties in addition to those shared by all Toeplitz matrices.\n",
    "\n",
    "Its eigenvalues and  eigenvectors are both easy to compute. \n",
    "\n",
    "In fact, its eiegenvectors are the Fourier modes (see more at https://en.wikipedia.org/wiki/Circulant_matrix#Eigenvectors_and_eigenvalues). \n",
    "\n",
    "Its determinant is also easy to compute.\n",
    "\n",
    "- The set of circulant matrices is a vector space.\n",
    "- If A and B are circulant, so is A + B.\n",
    "- If A and B are circulant, AB = BA and AB is circulant. In other words, circulant matrices are closed under matrix multiplication, and matrix multiplication is commutative.\n",
    "- It is also easy to solve systems with circulant matrices (see below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.linalg.solve_circulant([1, 2, 3], [1, 1, 1]) \n",
    "\n",
    "# The first vector is the 1st row of A and the second is b in Ax = b\n",
    "\n",
    "# the solution x is then returned by the above code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's instead solve A x = b the old fashioned way\n",
    "\n",
    "A = scipy.linalg.circulant([1, 2, 3])\n",
    "\n",
    "b = np.array([1, 1, 1])\n",
    "\n",
    "x = np.linalg.solve(A, b)\n",
    "\n",
    "x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Covariogram matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = scipy.linalg.circulant([1.0, .9, .81])\n",
    "print('V =', V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix factorization\n",
    "\n",
    "An important problem in linear algebra is matrix factorization\n",
    "\n",
    "Let's take a look at a few different decompositions:\n",
    "\n",
    "## LU-decomposition\n",
    "\n",
    "Suppose we have an n\\*n matrix A. We seek to compute the following:\n",
    "\n",
    "$$ A = LU $$\n",
    "\n",
    "where L is a lower triangular n\\*n matrix, and U is an upper triangular n\\*n matrix.\n",
    "\n",
    "Note that this isn't always possible (we'll discuss this below), in which case we decompose A as follows:\n",
    "\n",
    "$$ PA = LU $$\n",
    "where P is a permutation matrix.\n",
    "\n",
    "How do we find L and U?\n",
    "\n",
    "### Row echelon form\n",
    "Let's start with U. One way to obtain U is by computing the **row echelon form** of A. To compute the row echelon form, we start with A. The operations we are allowed to use are called **row operations**, and are described below:\n",
    "\n",
    "1. Interchange two rows of A.\n",
    "\n",
    "$$ \\begin{bmatrix}\n",
    "a & b & c \\\\\n",
    "d & e & f \\\\\n",
    "g & h & i\n",
    "\\end{bmatrix} \\implies \n",
    "\\begin{bmatrix}\n",
    "d & e & f \\\\\n",
    "a & b & c \\\\\n",
    "g & h & i\n",
    "\\end{bmatrix} $$\n",
    "\n",
    "2. Multiply any row of A by a scalar.\n",
    "\n",
    "$$ \\begin{bmatrix}\n",
    "a & b & c \\\\\n",
    "d & e & f \\\\\n",
    "g & h & i\n",
    "\\end{bmatrix} \\implies \n",
    "\\begin{bmatrix}\n",
    "2a & 2b & 2c \\\\\n",
    "d & e & f \\\\\n",
    "g & h & i\n",
    "\\end{bmatrix}  $$\n",
    "\n",
    "3. Add any two rows of A.\n",
    "\n",
    "$$ \\begin{bmatrix}\n",
    "a & b & c \\\\\n",
    "d & e & f \\\\\n",
    "g & h & i\n",
    "\\end{bmatrix} \\implies \n",
    "\\begin{bmatrix}\n",
    "a & b & c \\\\\n",
    "a+d & b+e & c+f \\\\\n",
    "g & h & i\n",
    "\\end{bmatrix} $$\n",
    "\n",
    "We want to use these operations to transform A into an upper triangular matrix U. Let's do an example:\n",
    "\n",
    "$$ \\begin{bmatrix}\n",
    "3 & 1\\\\\n",
    "-6 & -4\\\\\n",
    "\\end{bmatrix} \\implies \n",
    "\\begin{bmatrix}\n",
    "6 & 2 \\\\\n",
    "-6 & -4\\\\\n",
    "\\end{bmatrix} \\implies\n",
    "\\begin{bmatrix}\n",
    "6 & 2 \\\\\n",
    "0 & -2\\\\\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "First, we multiply the top row by 2. Next, we add the top row to the bottom row and store the result on the bottom row, changing its leftmost element to zero. Notice that the row operations are the same as the addition/elimination method of solving systems of equations. The procedure of eliminating unknowns from equations (or here, numbers from the leftmost column) is the same.\n",
    "\n",
    "Next, we need to find L. We can write out the sequence of row operations as matrix multiplications:\n",
    "\n",
    "$$ \\begin{bmatrix}\n",
    "3 & 1\\\\\n",
    "-6 & -4\\\\\n",
    "\\end{bmatrix} =\n",
    "\\begin{bmatrix}\n",
    "0.5 & 0 \\\\\n",
    "0 & 1\\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "6 & 2 \\\\\n",
    "-6 & -4\\\\\n",
    "\\end{bmatrix}\n",
    "= \n",
    "\\begin{bmatrix}\n",
    "0.5 & 0 \\\\\n",
    "0 & 1\\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "1 & 0 \\\\\n",
    "-1 & 1\\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "6 & 2 \\\\\n",
    "0 & -2\\\\\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "Multiplying all the matrices except U together gives us L:\n",
    "\n",
    "$$ L = \\begin{bmatrix}\n",
    "0.5 & 0 \\\\\n",
    "0 & 1\\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "1 & 0 \\\\\n",
    "-1 & 1\\\\\n",
    "\\end{bmatrix} =\n",
    "\\begin{bmatrix}\n",
    "0.5 & 0 \\\\\n",
    "-1 & 1\\\\\n",
    "\\end{bmatrix} $$\n",
    "\n",
    "This works because all of the matrices are lower triangular. However, this is not always the case. If we have to swap rows at any point, this process will not work because these matrices, called **permutation matrices**, are not triangular. In that case, we say that the matrix doesn't have an LU-decomposition. However, we can get around this issue if A by using the following equation:\n",
    "\n",
    "$$ PA = LU $$\n",
    "\n",
    "Why does this work? What we're saying here is that for the matrix A, there exists some permutation of the rows of A with an LU-decomposition. Proving this fact is relatively straightforward and involves showing that row operations can transform any matrix into an upper triangular one.\n",
    "\n",
    "\n",
    "Now, try computing the LU-decomposition for the following matrix:\n",
    "$$ \\begin{bmatrix}\n",
    "1 & 2 & 4 \\\\\n",
    "3 & 8 & 14 \\\\\n",
    "2 & 6 & 13\\\\\n",
    "\\end{bmatrix} $$\n",
    "\n",
    "### LU-decomposition using Python\n",
    "\n",
    "This snippet of code shows how to perform LU-decomposition on the matrix from the example. We use the LU function included in the scipy package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "A = np.array([np.array([3, 1]), np.array([-6, -4])])\n",
    "scipy.linalg.lu(A) # The first matrix returned is a permutation matrix, the second is L, and the third is U"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Properties of LU-decomposition\n",
    "\n",
    "Let's suppose that we have an n\\*n matrix A that has an LU-decomposition and we wish to solve the following system:\n",
    "\n",
    "$$ Ax = b $$\n",
    "\n",
    "where x and b are vectors of length n.\n",
    "\n",
    "Let's substitute in LU for A:\n",
    "$$ LUx = b $$\n",
    "\n",
    "Now let's define y = Ux:\n",
    "\n",
    "$$ y = Ux $$\n",
    "$$ Ly = b $$\n",
    "\n",
    "Notice that we went from a single complex linear system to two simpler ones, making the solution easier to compute. Both of these systems involve triangular matrices, so we can use substitution to solve them quickly. This is especially useful in cases where the matrix A remains the same, but we want to solve the system for different values of b."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QR-decomposition\n",
    "\n",
    "The QR-decomposition of an m\\*n matrix A is as follows. Note that this decomposition is not possible when m < n.\n",
    "\n",
    "$$ A = QR $$\n",
    "\n",
    "Here, Q is a m\\*m matrix with orthonormal columns, and R is an upper triangular m\\*n matrix.\n",
    "\n",
    "Let's start by finding Q.\n",
    "\n",
    "### Gram-Schmidt process\n",
    "\n",
    "The **Gram-Schmidt process** is a way to obtain a matrix with orthonormal columns from A. Let's look at how it works:\n",
    "\n",
    "1. Set n = 1.\n",
    "2. If n is 2 or more, subtract the orthogonal complement of columns n-1, n-2, ... , 1 from column n.\n",
    "3. Normalize column n.\n",
    "4. Increment n by 1.\n",
    "\n",
    "Let's look at an example to make this easier to understand:\n",
    "\n",
    "$$\\begin{bmatrix}\n",
    "1 & 4 \\\\\n",
    "0 & 3 \\\\\n",
    "1 & 0\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "We start by normalizing the first column.\n",
    "\n",
    "$$\\begin{bmatrix}\n",
    "\\frac{1}{\\sqrt{2}} & 4 \\\\\n",
    "0 & 3 \\\\\n",
    "\\frac{1}{\\sqrt{2}} & 0\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "Next, we take the component of column 2 that is orthogonal to column 1. In other words, we're looking for orth<sub>col1</sub> col2.\n",
    "\n",
    "$$\\begin{bmatrix}\n",
    "\\frac{1}{\\sqrt{2}} & 2\\\\\n",
    "0 & 3 \\\\\n",
    "\\frac{1}{\\sqrt{2}} & -2\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "Now, we normalize column 2.\n",
    "\n",
    "$$\\begin{bmatrix}\n",
    "\\frac{1}{\\sqrt{2}} & \\frac{2}{\\sqrt{17}}\\\\\n",
    "0 & \\frac{3}{\\sqrt{17}} \\\\\n",
    "\\frac{1}{\\sqrt{2}} & \\frac{-2}{\\sqrt{17}}\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "Note that Q is an m\\*m matrix. We need to add the 3rd unit vector to complete the basis of R<sup>3</sup>.\n",
    "\n",
    "$$\\begin{bmatrix}\n",
    "\\frac{1}{\\sqrt{2}} & \\frac{2}{\\sqrt{17}} & \\frac{-3}{\\sqrt{34}}\\\\\n",
    "0 & \\frac{3}{\\sqrt{17}} & \\frac{2\\sqrt{2}}{\\sqrt{17}}\\\\\n",
    "\\frac{1}{\\sqrt{2}} & \\frac{-2}{\\sqrt{17}} & \\frac{3}{\\sqrt{34}}\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "Now, we compute R by solving the following equation:\n",
    "\n",
    "$$ R = Q^TA $$\n",
    "\n",
    "Note that this process works for non-square matrices as well.\n",
    "\n",
    "### QR decomposition using Python\n",
    "\n",
    "We can use the qr function in the scipy module to compute the QR decomposition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "A = np.array([np.array([1, 4]),\n",
    "              np.array([0, 3]),\n",
    "              np.array([1, 0])])\n",
    "\n",
    "scipy.linalg.qr(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Properties of QR-decomposition\n",
    "\n",
    "- Q<sup>T</sup>Q = I. This holds because the columns of Q are orthonormal.\n",
    "- The solution of Rx = Q<sup>T</sup>b is the least squares solution to Ax=b.\n",
    "- The QR decomposition is not unique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cholesky decomposition\n",
    "\n",
    "The Cholesky decomposition only exists for certain matrices. Let's talk about their properties.\n",
    "\n",
    "1. The matrix must be **Hermitian**. This means that it is equal to its conjugate transpose (take the transpose, then take the conjugate of each element). Here, H denotes the conjugate transpose.\n",
    "\n",
    "$$ A = A^H $$\n",
    "\n",
    "2. The matrix must be **positive-definite**. An n\\*n Hermitian matrix is positive-definite if the following holds for every nonzero vector z of length n:\n",
    "\n",
    "$$ z^HMz > 0 $$\n",
    "\n",
    "Note that the expression above will always be real since M is a Hermitian matrix.\n",
    "\n",
    "Now that we've established some conditions, let's talk about what Cholesky decomposition is. The **Cholesky decomposition** of a Hermitian positive-definite matrix A is as follows:\n",
    "\n",
    "$$ A = LL^H $$\n",
    "\n",
    "Here, L is a lower-triangular matrix. Its diagonal entries are real, positive numbers. L<sup>H</sup> is its conjugate transpose. \n",
    "\n",
    "How do we compute the Cholesky decomposition?\n",
    "\n",
    "There are a few ways of doing this, but one of the least computationally intensive is to look at what happens when we multiply L and L<sup>H</sup>. We'll use a real 3x3 matrix to illustrate the process:\n",
    "\n",
    "$$ A = LL^H =\\begin{bmatrix}\n",
    "a & 0 & 0 \\\\\n",
    "b & c & 0 \\\\\n",
    "d & e & f \\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "a & b & d \\\\\n",
    "0 & c & e \\\\\n",
    "0 & 0 & f \\\\\n",
    "\\end{bmatrix}= \n",
    "\\begin{bmatrix}\n",
    "a^2 & ab & ad \\\\\n",
    "ab & b^2 + c^2 & bd + ce \\\\\n",
    "ad & bd + ce & d^2 + e^2 + f^2 \\\\\n",
    "\\end{bmatrix} $$\n",
    "\n",
    "We can now solve recursively for the entries of L from the coefficients of A. Note that we always take the positive square root to ensure that the elements on the main diagonal are positive.\n",
    "\n",
    "$$ L_{11} = \\sqrt{A_{11}} $$\n",
    "\n",
    "\n",
    "$$ L_{21} = \\frac{A_{21}}{L_{11}} $$\n",
    "\n",
    "\n",
    "$$ L_{22} =  \\sqrt{A_{22} - L_{21}^2} $$\n",
    "\n",
    "\n",
    "$$ L_{31} = \\frac{A_{31}}{L_{11}} $$\n",
    "\n",
    "\n",
    "$$ L_{32} = \\frac{A_{32} - L_{31}L_{21}}{L_{22}} $$\n",
    "\n",
    "\n",
    "$$ L_{33} = \\sqrt{A_{33} - L_{31}^2 - L_{32}^2} $$\n",
    "\n",
    "\n",
    "We can use this to create formulas for the entries of L. The following formulas give us the correct entries for L:\n",
    "\n",
    "$$ L_{j,j} = \\sqrt{A_{j,j} - \\sum\\limits_{k=1}^{j-1}{L_{j,k}L_{j,k}^*}} $$\n",
    "$$ L_{i,j} = \\frac{1}{L_{j,j}}\\left(A_{i,j} - \\sum\\limits_{k=1}^{j-1}{L_{i,k}L_{j,k}^*}\\right), i > j$$\n",
    "$$ 0, i < j $$\n",
    "\n",
    "Here, the * operation is the conjugate of a complex number. Note that these formulas also apply to any Hermitian psoitive-definite matrix A.\n",
    "\n",
    "Here's how to compute a Cholesky decomposition in Python. We simply use the built-in cholesky function in the scipy package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "A = np.array([np.array([3, 1]), \n",
    "              np.array([1, 4])])\n",
    "\n",
    "scipy.linalg.cholesky(A, lower=True) # If lower = False, the default value, this returns L^H instead of L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Properties of Cholesky decomposition\n",
    "\n",
    "- The Cholesky decomposition of a Hermitian positive-definite matrix is unique.\n",
    "- When A is real, L is also real and L<sup>H</sup> = L<sup>T</sup>.\n",
    "- If A is Hermitian and positive-definite, L is invertible.\n",
    "- The Cholesky decomposition can also be performed on Hermitian positive-semidefinite matrices, but it is not unique in that case. We also have to allow some of the diagonal entries of L to be zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diagonalization\n",
    "\n",
    "A square n\\*n matrix A is **diagonalizable** if its right eigenvectors form a basis of R<sup>n</sup>.\n",
    "\n",
    "The diagonalization of A is as follows:\n",
    "\n",
    "$$ A = PDP^{-1} $$\n",
    "\n",
    "Here, P is an invertible matrix, and D is a diagonal matrix.\n",
    "\n",
    "The elements on the main diagonal of D are the eigenvalues of A, and the columns of P are its eigenvectors. Let's do an example:\n",
    "\n",
    "$$ A = \\begin{bmatrix}\n",
    "3 & 2 \\\\\n",
    "3 & 4 \n",
    "\\end{bmatrix} $$\n",
    "\n",
    "Let's start by computing the eigenvalues and eigenvectors of A. We'll use numpy to do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "A = np.array([np.array([3, 2]),\n",
    "              np.array([3, 4])])\n",
    "\n",
    "eig = np.linalg.eig(A) # The first item returned is an array of eigenvalues, \n",
    "                       # and the second is an array of the corresponding eigenvectors\n",
    "    \n",
    "eig # Note that the eigenvectors are columns here, not rows, so (-0.71, -0.55) is not an eigenvector, but (-0.71, 0.71) is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that numpy returns eigenvectors of unit length.\n",
    "\n",
    "First, we check that the eigenvectors form a basis of R<sup>2</sup>. Since this is R<sup>2</sup>, we only need to check that our vectors are not colinear. The dot product of (-0.71, 0.71) and (-0.55, -0.83) is different from 1 (the product of their lengths), so the vectors are not colinear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "abs(np.dot(eig[1][:,0], eig[1][:,1])) # We take the absoulte value since length is always positive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's rescale our eigenvectors to make calculations easier. Note that this won't have an effect on our results.\n",
    "\n",
    "$$ (-0.71, 0.71) \\implies (-1, 1) $$\n",
    "$$ (-0.55, -0.83) \\implies (2, 3) $$\n",
    "\n",
    "Since this is the case, we know that A is diagonalizable.\n",
    "The eigenvalues are 1 and 6. Then\n",
    "\n",
    "$$ D = \\begin{bmatrix}\n",
    "1 & 0 \\\\\n",
    "0 & 6 \n",
    "\\end{bmatrix} $$\n",
    "\n",
    "Next, we want to find P. P is just a matrix of the eigenvectors. However, we have to make sure that we put the eigenvectors in the right order. In other words, our first column should be the eigenvector with eigenvalue 1, and the second should be the eigenvector with eigenvalue 6. In our example, we have\n",
    "\n",
    "$$ P = \\begin{bmatrix}\n",
    "1 & 2 \\\\\n",
    "-1 & 3 \n",
    "\\end{bmatrix} $$\n",
    "\n",
    "### Properties of diagonalization\n",
    "\n",
    "Taking powers of diagonal matrices is very easy. To compute D<sup>n</sup>, where D is diagonal, we can just take the nth power of each of the diagonal elements. For 2\\*2 matrices, it looks like this:\n",
    "\n",
    "$$ D = \\begin{bmatrix}\n",
    "a & 0 \\\\\n",
    "0 & b \n",
    "\\end{bmatrix} $$\n",
    "\n",
    "$$ D^n = \\begin{bmatrix}\n",
    "a^n & 0 \\\\\n",
    "0 & b^n \n",
    "\\end{bmatrix} $$\n",
    "\n",
    "This gives us an easy way to compute powers of diagonalizable matrices:\n",
    "\n",
    "$$ A^n = (PDP^{-1})^n = PD^nP^{-1} $$\n",
    "\n",
    "To prove this statement, we can use a proof by induction relying on the idea that\n",
    "\n",
    "$$ A^n = (PDP^{-1})^n = (PDP^{-1})^{n-1} (PDP^{-1}) = PD^{n-1}P^{-1}PDP^{-1} = PD^nP^{-1} $$\n",
    "\n",
    "Note that diagonalization is not unique.\n",
    "\n",
    "If A is symmetric and diagonalizable, we can write diagonalization as\n",
    "\n",
    "$$ A = PDP^T $$\n",
    "\n",
    "where P is an orthogonal matrix (i.e. its columns are orthonormal). This works because all the eigenvectors of a symmetric matrix are orthogonal if their eigenvalues are different, and therefore the inverse and the transpose of P are the same.\n",
    "\n",
    "The columns of P form an eigenbasis of R<sup>n</sup>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QZ-decomposition\n",
    "\n",
    "To apply QZ-decomposition, also called **generalized Schur decomposition**, we start with a square matrices A and B with complex entries. We factor as follows:\n",
    "\n",
    "$$ A = QSZ^* $$\n",
    "\n",
    "$$ B = QTZ^* $$\n",
    "\n",
    "Here, Q and Z are unitary matrices, and S and T are upper triangular. The * operation is the conjugate transpose.\n",
    "\n",
    "Let's break this down. A **unitary matrix** U is simply one for which\n",
    "\n",
    "$$ U^* = U^{-1} $$\n",
    "\n",
    "In other words, the inverse and conjugate transpose of a unitary matrix are the same.\n",
    "\n",
    "To compute a QZ decomposition in Python, we use scipy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "A = np.array([np.array([3, 2]),\n",
    "              np.array([9, 4])])\n",
    "\n",
    "B = np.array([np.array([3, 6]), \n",
    "              np.array([1, 4])])\n",
    "\n",
    "scipy.linalg.qz(A, B) # This returns S, T, Q, and Z in that order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Properties of QZ-decomposition\n",
    "\n",
    "- The QZ-decomposition is not unique.\n",
    "- If A and B are real, S is only quasi upper-triangular.\n",
    "- All complex pairs of matrices have a QZ-decomposition.\n",
    "- The ratio of the diagonal elements of S and T have the following property:\n",
    "\n",
    "$$ \\frac{S_{ii}}{T_{ii}} = \\lambda $$\n",
    "$$ Ax = \\lambda Bx $$\n",
    "\n",
    "The vectors x are the vectors in Z here (the analog of right eigenvectors)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Control Problem\n",
    "\n",
    "Here we'll use an $LU$ decomposition to solve a control problem.  \n",
    "\n",
    "Let $ L $ be the **lag operator**, so that, for sequence $ \\{x_t\\} $ we have $ L x_t = x_{t-1} $.\n",
    "\n",
    "More generally, let $ L^k x_t = x_{t-k} $ with $ L^0 x_t = x_t $ and\n",
    "\n",
    "$$\n",
    "d(L) = d_0 + d_1 L+ \\ldots + d_m L^m\n",
    "$$\n",
    "\n",
    "where $ d_0, d_1, \\ldots, d_m $ is a given scalar sequence.\n",
    "\n",
    "Consider the discrete-time control problem\n",
    "\n",
    "\n",
    "<a id='equation-oneone'></a>\n",
    "$$\n",
    "\\max_{\\{y_t\\}}\n",
    "\\lim_{N \\to \\infty} \\sum^N_{t=0} \\beta^t\\,\n",
    "\\left\\{\n",
    "     a_t y_t - {1 \\over 2}\\, hy^2_t - {1 \\over 2} \\,\n",
    "         \\left[ d(L)y_t \\right]^2\n",
    "\\right\\}, \\tag{1}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "- $ h $ is a positive parameter and $ \\beta \\in (0,1) $ is a discount factor.  \n",
    "- $ \\{a_t\\}_{t \\geq 0} $ is a sequence of exponential order less than $ \\beta^{-1/2} $, by which we mean $ \\lim_{t \\rightarrow \\infty} \\beta^{\\frac{t}{2}} a_t = 0 $.  \n",
    "\n",
    "\n",
    "Maximization in [(1)](#equation-oneone) is subject to  initial conditions for $ y_{-1}, y_{-2} \\ldots, y_{-m} $.\n",
    "\n",
    "Maximization is over infinite sequences $ \\{y_t\\}_{t \\geq 0} $.\n",
    "\n",
    "### Example\n",
    "\n",
    "The formulation of the LQ problem given above is broad enough to encompass\n",
    "many useful models.\n",
    "\n",
    "As a simple illustration, recall that in [LQ Control: Foundations](https://python-intro.quantecon.org/lqcontrol.html) we consider a monopolist facing stochastic demand\n",
    "shocks and adjustment costs.\n",
    "\n",
    "Let’s consider a deterministic version of this problem, where the monopolist\n",
    "maximizes the discounted sum\n",
    "\n",
    "$$\n",
    "\\sum_{t=0}^{\\infty} \\beta^t \\pi_t\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "\\pi_t = p_t q_t - c q_t - \\gamma (q_{t+1} - q_t)^2\n",
    "\\quad \\text{with} \\quad\n",
    "p_t = \\alpha_0 - \\alpha_1 q_t + d_t\n",
    "$$\n",
    "\n",
    "In this expression, $ q_t $ is output, $ c $ is average cost of production, and $ d_t $ is a demand shock.\n",
    "\n",
    "The term $ \\gamma (q_{t+1} - q_t)^2 $ represents adjustment costs.\n",
    "\n",
    "You will be able to confirm that the objective function can be rewritten as [(1)](#equation-oneone) when\n",
    "\n",
    "- $ a_t := \\alpha_0 + d_t - c $  \n",
    "- $ h := 2 \\alpha_1 $  \n",
    "- $ d(L) := \\sqrt{2 \\gamma}(I - L) $  \n",
    "\n",
    "\n",
    "Further examples of this problem for factor demand, economic growth, and government policy problems are given in ch. IX of [[Sar87]](https://python-programming.quantecon.org/zreferences.html#sargent1987)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finite Horizon Theory\n",
    "\n",
    "We first study a finite $ N $ version of the problem.\n",
    "\n",
    "Later we will study an infinite horizon problem solution as a limiting version of a finite horizon problem.\n",
    "\n",
    "(This will require being careful because the limits as $ N \\to \\infty $ of the necessary and sufficient conditions for maximizing finite $ N $ versions of [(1)](#equation-oneone)\n",
    "are not sufficient for maximizing [(1)](#equation-oneone))\n",
    "\n",
    "We begin by\n",
    "\n",
    "1. fixing $ N > m $,  \n",
    "1. differentiating the finite version of [(1)](#equation-oneone) with respect to $ y_0, y_1, \\ldots, y_N $, and  \n",
    "1. setting these derivatives to zero.  \n",
    "\n",
    "\n",
    "For $ t=0, \\ldots, N-m $ these first-order necessary conditions are the\n",
    "*Euler equations*.\n",
    "\n",
    "For $ t = N-m + 1, \\ldots, N $, the first-order conditions are a set of\n",
    "*terminal conditions*.\n",
    "\n",
    "Consider the term\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "J\n",
    "& = \\sum^N_{t=0} \\beta^t [d(L) y_t] [d(L) y_t]\n",
    "\\\\\n",
    "& = \\sum^N_{t=0}\n",
    "    \\beta^t \\, (d_0 \\, y_t + d_1 \\, y_{t-1} + \\cdots + d_m \\, y_{t-m}) \\,\n",
    "               (d_0 \\, y_t + d_1 \\, y_{t-1} + \\cdots  + d_m\\, y_{t-m})\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Differentiating $ J $ with respect to $ y_t $ for\n",
    "$ t=0,\\ 1,\\ \\ldots,\\ N-m $ gives\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "{\\partial {J} \\over \\partial y_t}\n",
    "   & = 2 \\beta^t \\, d_0 \\, d(L)y_t +\n",
    "       2 \\beta^{t+1} \\, d_1\\, d(L)y_{t+1} + \\cdots +\n",
    "       2 \\beta^{t+m}\\, d_m\\, d(L) y_{t+m} \\\\\n",
    "   & = 2\\beta^t\\, \\bigl(d_0 + d_1 \\, \\beta L^{-1} + d_2 \\, \\beta^2\\, L^{-2} +\n",
    "       \\cdots + d_m \\, \\beta^m \\, L^{-m}\\bigr)\\, d (L) y_t\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "We can write this more succinctly as\n",
    "\n",
    "\n",
    "<a id='equation-onetwo'></a>\n",
    "$$\n",
    "{\\partial {J} \\over \\partial y_t}\n",
    "    = 2 \\beta^t \\, d(\\beta L^{-1}) \\, d (L) y_t \\tag{2}\n",
    "$$\n",
    "\n",
    "Differentiating $ J $ with respect to $ y_t $ for $ t = N-m + 1, \\ldots, N $ gives\n",
    "\n",
    "\n",
    "<a id='equation-onethree'></a>\n",
    "$$\n",
    "\\begin{aligned}\n",
    " {\\partial J \\over \\partial y_N}\n",
    " &= 2 \\beta^N\\, d_0 \\, d(L) y_N \\cr\n",
    "   {\\partial J \\over \\partial y_{N-1}}\n",
    " &= 2\\beta^{N-1} \\,\\bigl[d_0 + \\beta \\,\n",
    "   d_1\\, L^{-1}\\bigr] \\, d(L)y_{N-1} \\cr\n",
    "   \\vdots\n",
    " & \\quad \\quad \\vdots \\cr\n",
    "   {\\partial {J} \\over \\partial y_{N-m+1}}\n",
    " &= 2 \\beta^{N-m+1}\\,\\bigl[d_0 + \\beta\n",
    "   L^{-1} \\,d_1 + \\cdots + \\beta^{m-1}\\, L^{-m+1}\\, d_{m-1}\\bigr]  d(L)y_{N-m+1}\n",
    "\\end{aligned} \\tag{3}\n",
    "$$\n",
    "\n",
    "With these preliminaries under our belts, we are ready to differentiate [(1)](#equation-oneone).\n",
    "\n",
    "Differentiating [(1)](#equation-oneone) with respect to $ y_t $ for $ t=0, \\ldots, N-m $ gives the Euler equations\n",
    "\n",
    "\n",
    "<a id='equation-onefour'></a>\n",
    "$$\n",
    "\\bigl[h+d\\,(\\beta L^{-1})\\,d(L)\\bigr] y_t = a_t,\n",
    "\\quad t=0,\\, 1,\\, \\ldots, N-m \\tag{4}\n",
    "$$\n",
    "\n",
    "The system of equations [(4)](#equation-onefour) forms  a  $ 2 \\times m $ order linear *difference\n",
    "equation* that must hold for the values of $ t $ indicated.\n",
    "\n",
    "Differentiating [(1)](#equation-oneone) with respect to $ y_t $ for $ t = N-m + 1, \\ldots, N $ gives the terminal conditions\n",
    "\n",
    "\n",
    "<a id='equation-onefive'></a>\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\beta^N  (a_N - hy_N - d_0\\,d(L)y_N)\n",
    "&= 0  \\cr\n",
    "  \\beta^{N-1} \\left(a_{N-1}-hy_{N-1}-\\Bigl(d_0 + \\beta \\, d_1\\,\n",
    "L^{-1}\\Bigr)\\, d(L)\\, y_{N-1}\\right)\n",
    "& = 0 \\cr\n",
    " \\vdots & \\vdots\\cr\n",
    "\\beta^{N-m+1} \\biggl(a_{N-m+1} - h y_{N-m+1} -(d_0+\\beta L^{-1}\n",
    "d_1+\\cdots\\  +\\beta^{m-1} L^{-m+1} d_{m-1}) d(L) y_{N-m+1}\\biggr)\n",
    "& = 0\n",
    "\\end{aligned} \\tag{5}\n",
    "$$\n",
    "\n",
    "In the finite $ N $ problem, we want simultaneously to solve [(4)](#equation-onefour) subject to the $ m $ initial conditions\n",
    "$ y_{-1}, \\ldots, y_{-m} $ and the $ m $ terminal conditions\n",
    "[(5)](#equation-onefive).\n",
    "\n",
    "These conditions uniquely pin down the solution of the finite $ N $ problem.\n",
    "\n",
    "That is, for the finite $ N $ problem,\n",
    "conditions [(4)](#equation-onefour) and [(5)](#equation-onefive) are necessary and sufficient for a maximum,\n",
    "by concavity of the objective function.\n",
    "\n",
    "Next, we describe how to obtain the solution using matrix methods.\n",
    "\n",
    "\n",
    "<a id='fdlq'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix Methods\n",
    "\n",
    "Let’s look at how linear algebra can be used to tackle and shed light on the finite horizon LQ control problem.\n",
    "\n",
    "#### A Single Lag Term\n",
    "\n",
    "Let’s begin with the special case in which $ m=1 $.\n",
    "\n",
    "We want to solve the system of $ N+1 $ linear equations\n",
    "\n",
    "\n",
    "<a id='equation-oneff'></a>\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\bigl[h & + d\\, (\\beta L^{-1})\\, d\\, (L) ] y_t = a_t, \\quad\n",
    "t = 0,\\ 1,\\ \\ldots,\\, N-1\\cr\n",
    "\\beta^N & \\bigl[a_N-h\\, y_N-d_0\\, d\\, (L) y_N\\bigr] = 0\n",
    "\\end{aligned} \\tag{6}\n",
    "$$\n",
    "\n",
    "where $ d(L) = d_0 + d_1 L $.\n",
    "\n",
    "These equations are to be solved for\n",
    "$ y_0, y_1, \\ldots, y_N $ as functions of\n",
    "$ a_0, a_1, \\ldots,  a_N $ and $ y_{-1} $.\n",
    "\n",
    "Let\n",
    "\n",
    "$$\n",
    "\\phi (L)\n",
    "= \\phi_0 + \\phi_1 L + \\beta \\phi_1 L^{-1}\n",
    "= h + d (\\beta L^{-1}) d(L)\n",
    "= (h + d_0^2 + d_1^2) + d_1 d_0 L+ d_1 d_0 \\beta L^{-1}\n",
    "$$\n",
    "\n",
    "Then we can represent [(6)](#equation-oneff) as the matrix equation\n",
    "\n",
    "\n",
    "<a id='equation-onefourfive'></a>\n",
    "$$\n",
    "\\left[\n",
    "    \\begin{matrix}\n",
    "        (\\phi_0-d_1^2) & \\phi_1 & 0 & 0 & \\ldots & \\ldots & 0 \\cr\n",
    "        \\beta \\phi_1 & \\phi_0 & \\phi_1 & 0 & \\ldots & \\dots & 0 \\cr\n",
    "        0 & \\beta \\phi_1 & \\phi_0 & \\phi_1 & \\ldots & \\ldots & 0 \\cr\n",
    "        \\vdots &\\vdots & \\vdots & \\ddots & \\vdots & \\vdots & \\vdots \\cr\n",
    "        0 & \\ldots & \\ldots & \\ldots & \\beta \\phi_1 & \\phi_0 &\\phi_1 \\cr\n",
    "        0 & \\ldots & \\ldots & \\ldots & 0 & \\beta \\phi_1 & \\phi_0\n",
    "    \\end{matrix}\n",
    "\\right]\n",
    "\\left [\n",
    "    \\begin{matrix}\n",
    "        y_N \\cr y_{N-1} \\cr y_{N-2} \\cr \\vdots \\cr\n",
    "        y_1 \\cr y_0\n",
    "    \\end{matrix}\n",
    "\\right ] =\n",
    "\\left[\n",
    "\\begin{matrix}\n",
    "    a_N \\cr a_{N-1} \\cr a_{N-2} \\cr \\vdots \\cr a_1 \\cr\n",
    "    a_0 - \\phi_1 y_{-1}\n",
    "\\end{matrix}\n",
    "\\right] \\tag{7}\n",
    "$$\n",
    "\n",
    "or\n",
    "\n",
    "\n",
    "<a id='equation-onefoursix'></a>\n",
    "$$\n",
    "W\\bar y = \\bar a \\tag{8}\n",
    "$$\n",
    "\n",
    "Notice how we have chosen to arrange the $ y_t $’s in reverse\n",
    "time order.\n",
    "\n",
    "The matrix $ W $ on the left side of [(7)](#equation-onefourfive) is “almost” a\n",
    "[Toeplitz matrix](https://en.wikipedia.org/wiki/Toeplitz_matrix) (where each\n",
    "descending diagonal is constant).\n",
    "\n",
    "There are two sources of deviation from the  form  of a Toeplitz matrix\n",
    "\n",
    "1. The first element differs from the remaining diagonal elements, reflecting the terminal condition.  \n",
    "1. The sub-diagonal elements equal $ \\beta $ time the super-diagonal elements.  \n",
    "\n",
    "\n",
    "The solution of [(8)](#equation-onefoursix) can be expressed in the form\n",
    "\n",
    "\n",
    "<a id='equation-onefourseven'></a>\n",
    "$$\n",
    "\\bar y = W^{-1} \\bar a \\tag{9}\n",
    "$$\n",
    "\n",
    "which represents each element $ y_t $ of $ \\bar y $ as a function of the entire vector $ \\bar a $.\n",
    "\n",
    "That is, $ y_t $ is a function of past, present, and future values of $ a $’s, as well as of the initial condition $ y_{-1} $.\n",
    "\n",
    "\n",
    "#### An Alternative Representation\n",
    "\n",
    "An alternative way to express the solution to [(7)](#equation-onefourfive) or\n",
    "[(8)](#equation-onefoursix) is in so-called **feedback-feedforward** form.\n",
    "\n",
    "The idea here is to find a solution expressing $ y_t $ as a function of *past* $ y $’s and *current* and *future* $ a $’s.\n",
    "\n",
    "To achieve this solution, one can use an [LU decomposition](https://en.wikipedia.org/wiki/LU_decomposition) of $ W $.\n",
    "\n",
    "There always exists a decomposition of $ W $ of the form $ W= LU $\n",
    "where\n",
    "\n",
    "- $ L $ is an $ (N+1) \\times (N+1) $ lower triangular matrix.  \n",
    "- $ U $ is an $ (N+1) \\times (N+1) $ upper triangular matrix.  \n",
    "\n",
    "\n",
    "The factorization can be normalized so that the diagonal elements of $ U $ are unity.\n",
    "\n",
    "Using the LU representation in [(9)](#equation-onefourseven), we obtain\n",
    "\n",
    "\n",
    "<a id='equation-onefournine'></a>\n",
    "$$\n",
    "U \\bar y = L^{-1} \\bar a \\tag{10}\n",
    "$$\n",
    "\n",
    "Since $ L^{-1} $ is lower triangular, this representation expresses\n",
    "$ y_t $ as a function of\n",
    "\n",
    "- lagged $ y $’s (via the term $ U \\bar y $), and  \n",
    "- current and future $ a $’s (via the term $ L^{-1} \\bar a $)  \n",
    "\n",
    "\n",
    "Because there are zeros everywhere in the matrix\n",
    "on the left of [(7)](#equation-onefourfive) except on the diagonal, super-diagonal, and\n",
    "sub-diagonal, the $ LU $ decomposition takes\n",
    "\n",
    "- $ L $ to be zero except in the diagonal  and the leading sub-diagonal.  \n",
    "- $ U $ to be zero except on the diagonal and the super-diagonal.  \n",
    "\n",
    "\n",
    "Thus, [(10)](#equation-onefournine) has the form\n",
    "\n",
    "$$\n",
    "\\left[\n",
    "\\begin{matrix}\n",
    "    1& U_{12} & 0 & 0 & \\ldots & 0 & 0 \\cr\n",
    "    0 & 1 & U_{23} & 0 & \\ldots & 0 & 0 \\cr\n",
    "    0 & 0 & 1 & U_{34} & \\ldots & 0 & 0 \\cr\n",
    "    0 & 0 & 0 & 1 & \\ldots & 0 & 0\\cr\n",
    "    \\vdots & \\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\vdots\\cr\n",
    "    0 & 0 & 0 & 0 & \\ldots & 1 & U_{N,N+1} \\cr\n",
    "    0 & 0 & 0 & 0 & \\ldots & 0 & 1\n",
    "\\end{matrix}\n",
    "\\right] \\ \\ \\\n",
    "\\left[\n",
    "\\begin{matrix}\n",
    "    y_N \\cr y_{N-1} \\cr y_{N-2} \\cr y_{N-3} \\cr \\vdots \\cr y_1 \\cr y_0\n",
    "\\end{matrix}\n",
    "\\right] =\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\quad\n",
    "\\left[\n",
    "\\begin{matrix}\n",
    "    L^{-1}_{11} & 0 & 0 & \\ldots & 0 \\cr\n",
    "    L^{-1}_{21} & L^{-1}_{22} & 0 & \\ldots & 0 \\cr\n",
    "    L^{-1}_{31} & L^{-1}_{32} & L^{-1}_{33}& \\ldots & 0 \\cr\n",
    "    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots\\cr\n",
    "    L^{-1}_{N,1} & L^{-1}_{N,2} & L^{-1}_{N,3} & \\ldots & 0 \\cr\n",
    "    L^{-1}_{N+1,1} & L^{-1}_{N+1,2} & L^{-1}_{N+1,3} & \\ldots &\n",
    "    L^{-1}_{N+1\\, N+1}\n",
    "\\end{matrix}\n",
    "\\right]\n",
    "\\left[\n",
    "\\begin{matrix}\n",
    "    a_N \\cr a_{N-1} \\cr a_{N-2} \\cr \\vdots \\cr a_1 \\cr a_0 -\n",
    "    \\phi_1 y_{-1}\n",
    "\\end{matrix}\n",
    "\\right ]\n",
    "$$\n",
    "\n",
    "where $ L^{-1}_{ij} $ is the $ (i,j) $ element of $ L^{-1} $ and $ U_{ij} $ is the $ (i,j) $ element of $ U $.\n",
    "\n",
    "Note how the left side for a given $ t $ involves  $ y_t $ and one lagged value $ y_{t-1} $ while the right side involves all future values of the forcing process $ a_t, a_{t+1}, \\ldots, a_N $.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's generate a simple LQ control example\n",
    "\n",
    "and solve it with the $LU$ decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1=np.array([3.0, .9, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0])\n",
    "# c1 = np.array([1.0, .8, 0.0])\n",
    "a=np.ones(8)\n",
    "\n",
    "d1 = .5\n",
    "ym1 = 0.0 \n",
    "a[7] = a[7] - ym1\n",
    "r1 = c1\n",
    "V1= scipy.linalg.toeplitz(np.transpose(c1),r1)\n",
    "V1[0,0] = V1[0,0] - d1**2\n",
    "print('V1 = ', V1)\n",
    "V1inv = scipy.linalg.inv(V1)\n",
    "P1, L1, U1 = scipy.linalg.lu(V1)\n",
    "L1inv=scipy.linalg.inv(L1)\n",
    "print('L1 = ', L1)\n",
    "print('L1inv = ', L1inv)\n",
    "print('U1 = ', U1)\n",
    "print('P1 = ', P1)\n",
    "Vinva = V1inv@a\n",
    "print('Vinva =', Vinva)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Covariogram as Toeplitz matrix\n",
    "\n",
    "\n",
    "\n",
    "Just specify one side of the autocovariogram; equate  both the first row and the first column to it\n",
    "then go.\n",
    "\n",
    "We use this to compute finite dimensional moving average and autoregressive representations\n",
    "\n",
    "Use this as a laboratory to show how you can violate \"positive definiteness\" of the autocovariance sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1 = np.array([1.0, .9, .81, .9**3, .9**4, .9**5])\n",
    "r1 = c1\n",
    "V0= scipy.linalg.toeplitz(np.transpose(c1),r1)\n",
    "print('V0 =', V0)\n",
    "v,d = np.linalg.eig(V0)\n",
    "print('v =', v)\n",
    "\n",
    "F = scipy.linalg.cholesky(V0)\n",
    "\n",
    "print('F.T = ', F.T)\n",
    "\n",
    "FF = np.dot(F.T,F)\n",
    "\n",
    "FF = F.T@F\n",
    "\n",
    "V0 - FF\n",
    "\n",
    "AR = scipy.linalg.inv(F.T)   # Moving average representation \n",
    "\n",
    "print(\"autoregressive representation\")\n",
    "print(\"AR = \", AR)\n",
    "\n",
    "print(\"moving average representation\")\n",
    "print(\"MA = \", F.T)\n",
    "print(\"notice striking first column and how it relates to the covariance matrix -- there is a theorem here\")\n",
    "print('V0 =', V0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2.065/2.294"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ".393/.435"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
