{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Hierarchical Regression: Elasticities\n",
    "\n",
    "**Today**\n",
    "\n",
    "* Price elasticity in a retail setting\n",
    "* Orange juice data\n",
    "* Hierarchical models\n",
    "* Centered vs non-centered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import arviz as az\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pymc3 as pm\n",
    "\n",
    "np.set_printoptions(precision=4, linewidth=150)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Elasticities\n",
    "\n",
    "We will start today by discussing the price elasticity of demand:\n",
    "\n",
    "\\begin{align*}\n",
    "  \\eta_i &= \\frac{\\Delta q / q}{\\Delta p / p} = \\frac{dq / q}{dp / p}\n",
    "\\end{align*}\n",
    "\n",
    "What sign should $\\eta_i$ take?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Most individuals buy less of a good as the price increases $\\rightarrow \\eta_i < 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Constant elasticity demand function\n",
    "\n",
    "We will use the following demand function in today's lecture\n",
    "\n",
    "\\begin{align*}\n",
    "  q = \\alpha p^{\\eta}\n",
    "\\end{align*}\n",
    "\n",
    "One could motivate it theoretically, but we'll skip those steps and refer the interested student to the book \"Economics and Consumer Behavior\" by Angus Deaton and John Muellbauer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Taking the log of the demand function and take the derivative w.r.t $p$ gives us\n",
    "\n",
    "\\begin{align*}\n",
    "  \\log(q) &= \\log(\\alpha) + \\eta \\log(p) \\\\\n",
    "  \\frac{1}{q} \\frac{\\partial q}{\\partial p} &= \\eta \\frac{1}{p} \\\\\n",
    "  \\rightarrow \\frac{\\partial q / q}{\\partial p / p} &= \\eta \n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Goals of a retail store\n",
    "\n",
    "The owners of retail stores are typically interested in maximizing the amount they earn.\n",
    "\n",
    "A simplified view of how a store could achieve this is to choose a price for their goods that maximizes today's profits (this ignores dynamic opportunities and other important considerations).\n",
    "\n",
    "\\begin{align*}\n",
    "  \\pi &= p q - c q \\\\\n",
    "  \\pi &= \\alpha p^{1 + \\eta} - c \\alpha p^{\\eta} \\\\\n",
    "  \\frac{\\partial \\pi}{\\partial p} &= \\alpha (1 + \\eta) p^{\\eta} - c \\alpha \\eta p^{\\eta-1} = 0\\\\\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\\begin{align*}\n",
    "  p &= c \\frac{\\eta}{1 + \\eta}\n",
    "\\end{align*}\n",
    "\n",
    "Two cases:\n",
    "\n",
    "- $\\eta < -1$: This results in $p > 0$ because $1 + \\eta < 0 \\rightarrow \\frac{\\eta}{1 + \\eta} > 0$\n",
    "- $\\eta \\geq -1$: This creates problems for the profit maximization of this particular demand function and results in $p < 0$... The easiest way forward is to acknowledge that our demand equation has short-comings and that the elasticities faced by retail stores are not likely constant -- Maybe we justify this by claiming that the constant elasticity demand model could be a reasonable local approximation to a more complex demand model that we aren't able to specify."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### At what level to think about an elasticity?\n",
    "\n",
    "Imagine you own 5 stores in different geographies and the stores each sell the same set of goods. If you're the owner of these stores, you need to consider how to think about elasticities... \n",
    "\n",
    "Are they shared across products? Across geographies? Not shared at all?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Global level**\n",
    "\n",
    "All items in your store across all geographies have the same elasticity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Product level**\n",
    "\n",
    "Each item has an elasticity but that elasticity is the same across each store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Store level**\n",
    "\n",
    "All items in each store have the same elasticity but that elasticity differs by store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**(Product $\\times$ Store) level**\n",
    "\n",
    "Each product has a different elasticity in every store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Trade-off\n",
    "\n",
    "The more you allow your elasticities to differ by product and geography, the more accurately you will capture the differences across products and geographies...\n",
    "\n",
    "But the more you allow for differences, the less data you have to accurately estimate each elasticity parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Cross-price elasticities\n",
    "\n",
    "Typically, we would also consider how a price change in one good could affect the purchases of another good. For example, raising the price of orange juice might result in individuals choosing to purchase less orange juice AND more apple juice.\n",
    "\n",
    "We are going to skip cross-price elasticities but the paper that motivated this exercise (which is in this handouts folder) did estimate cross-price elasticities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data: Dominick's Orange Juice\n",
    "\n",
    "This data was originally a part of the [Dominick's Finer Foods](https://www.chicagobooth.edu/research/kilts/datasets/dominicks) data collection which ran from 1989 to 1994.\n",
    "\n",
    "We've followed work by [Greg Allenby](https://fisher.osu.edu/people/allenby.1) and have selected a subset of products. The subset includes 11 different orange juice selections -- [This data](https://cran.r-project.org/web/packages/bayesm/bayesm.pdf#page=40&zoom=100,132,89) is also available through the `bayesm` R package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Data description\n",
    "\n",
    "There are 11 orange juices in the full dataset. They include:\n",
    "\n",
    "1. Tropicana Premium 64 oz (premium)\n",
    "2. Tropicana Premium 96 oz (premium)\n",
    "3. Floridaâ€™s Natural 64 oz (premium)\n",
    "4. Tropicana 64 oz (national)\n",
    "5. Minute Maid 64 oz (national)\n",
    "6. Minute Maid 96 oz (national)\n",
    "7. Citrus Hill 64 oz (national)\n",
    "8. Tree Fresh 64 oz (national)\n",
    "9. Florida Gold 64 oz (national)\n",
    "10. Dominicks 64 oz (store)\n",
    "11. Dominicks 128 oz (store)\n",
    "\n",
    "The prices in the dataset are reported in \\\\$/oz\n",
    "\n",
    "The quantity sold are reported in logs in the `logmove` variable\n",
    "\n",
    "There are approximately 80 stores in the dataset with between 85 and 125 weeks of data for each store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "oj_raw = pd.read_csv(\"oj.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "oj_raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Get a single log price per row**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "oj = oj_raw.copy()\n",
    "\n",
    "oj.loc[:, \"logprice\"] = oj.apply(\n",
    "    lambda x: np.log(x[f'price{x[\"brand\"].astype(int)}']), axis=\"columns\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Label products premium/national/store**\n",
    "\n",
    "* 0 -> premium\n",
    "* 1 -> national\n",
    "* 2 -> store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "labeler = {\n",
    "    1: 0,\n",
    "    2: 0,\n",
    "    3: 0,\n",
    "    4: 1,\n",
    "    5: 1,\n",
    "    6: 1,\n",
    "    7: 1,\n",
    "    8: 1,\n",
    "    9: 1,\n",
    "    10: 2,\n",
    "    11: 2,\n",
    "}\n",
    "\n",
    "oj.loc[:, \"quality\"] = oj.loc[:, \"brand\"].map(lambda x: labeler[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Shrink dataset**\n",
    "\n",
    "We would like to be able to sample from our posterior relatively quickly for this class, so we're going to discard some of the data...\n",
    "\n",
    "We've run other (more complex) Bayesian models on the data successfully but it was relatively expensive time-wise -- The computational time of Bayesian models can sometimes be a drawback to doing large-scale work with them (but there are lots of great \"approximate Bayesian methods\" being developed that help combat this)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prods_to_keep = [1, 3, 4, 5, 7, 8, 10]\n",
    "\n",
    "oj = oj.query(\n",
    "    \"store < 33 &\"  # Only use 10 stores\n",
    "    \"(brand in @prods_to_keep)\"\n",
    "    \"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Determine product/store/quality/week indexers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Determine the number of products, stores, and weeks\n",
    "nproducts = oj.loc[:, \"brand\"].nunique()\n",
    "nstores = oj.loc[:, \"store\"].nunique()\n",
    "nquality = oj.loc[:, \"quality\"].nunique()\n",
    "nweeks = oj.loc[:, \"week\"].nunique()\n",
    " \n",
    "# Convert things into index references\n",
    "brand_mapper = dict(zip(oj[\"brand\"].unique(), range(nproducts)))\n",
    "store_mapper = dict(zip(oj[\"store\"].unique(), range(nstores)))\n",
    "quality_mapper = dict(zip(oj[\"quality\"].unique(), range(nquality)))\n",
    "week_mapper = dict(zip(oj[\"week\"].unique(), range(nweeks)))\n",
    "\n",
    "# Add indexer columns\n",
    "oj.loc[:, \"brand_idx\"] = oj.loc[:, \"brand\"].replace(brand_mapper)\n",
    "oj.loc[:, \"store_idx\"] = oj.loc[:, \"store\"].replace(store_mapper)\n",
    "oj.loc[:, \"quality_idx\"] = oj.loc[:, \"quality\"].replace(quality_mapper)\n",
    "oj.loc[:, \"week_idx\"] = oj.loc[:, \"week\"].replace(week_mapper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Discard unneeded columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "cols_to_keep = [\n",
    "    \"store_idx\",\n",
    "    \"store\",\n",
    "    \"brand_idx\",\n",
    "    \"brand\",\n",
    "    \"quality_idx\",\n",
    "    \"quality\",\n",
    "    \"week_idx\",\n",
    "    \"logmove\",\n",
    "    \"logprice\"\n",
    "]\n",
    "\n",
    "df = oj.loc[:, cols_to_keep].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bayesian Models for Orange Juice Demand\n",
    "\n",
    "We now proceed to develop and explore a sequence of Bayesian models for orange juice demand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Fully pooled\n",
    "\n",
    "In the fully pooled model, we will treat all products and stores as if they were the same.\n",
    "\n",
    "The model can be written as:\n",
    "\n",
    "\\begin{align*}\n",
    "  \\log(q_{i, s, t}) &= \\alpha + \\eta \\log(p_{i, s, t}) + \\sigma \\varepsilon_{i, s, t} \\\\\n",
    "  \\alpha &\\sim N(1, 10) \\\\\n",
    "  \\eta &\\sim N(-1, 5) \\\\\n",
    "  \\sigma &\\sim \\text{HalfStudentT}(10, 5)\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Prep data\n",
    "log_q = df[\"logmove\"].to_numpy()\n",
    "log_p = df[\"logprice\"].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "m_fp = pm.Model()\n",
    "\n",
    "with m_fp:\n",
    "    # Data\n",
    "    _log_q = pm.Data(\"log_q\", log_q)\n",
    "    _log_p = pm.Data(\"log_p\", log_p)\n",
    "\n",
    "    # Priors\n",
    "    alpha = pm.Normal(\"alpha\", 1, 10)\n",
    "    eta = pm.Normal(\"eta\", -1, 5)\n",
    "    sigma = pm.HalfStudentT(\"sigma\", nu=10, sigma=5)\n",
    "\n",
    "    # Likelihood\n",
    "    ll = pm.Normal(\n",
    "        \"ll\", alpha + eta*_log_p, sigma, observed=_log_q\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Sample from the posterior**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "with m_fp:\n",
    "    traces_fp = pm.sample(2000, tune=1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "with m_fp:\n",
    "    az.plot_trace(traces_fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Diagnostics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with m_fp:\n",
    "    ess = az.ess(traces_fp, relative=True)\n",
    "    rhat = az.rhat(traces_fp)\n",
    "\n",
    "print(\"Effective Sample Size (min across parameters)\")\n",
    "print(f\"\\talpha: {ess['alpha'].values.min()}\")\n",
    "print(f\"\\teta: {ess['eta'].values.min()}\")\n",
    "print(f\"\\tsigma: {ess['sigma'].values.min()}\")\n",
    "print(\"rhat (max across parameters)\")\n",
    "print(f\"\\talpha: {rhat['alpha'].values.max()}\")\n",
    "print(f\"\\teta: {rhat['eta'].values.max()}\")\n",
    "print(f\"\\tsigma: {rhat['sigma'].values.max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Sampling posterior predictive**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "with m_fp:\n",
    "    spp_fp = pm.sample_posterior_predictive(traces_fp, 500)\n",
    "\n",
    "spp_fp_logmove = spp_fp[\"ll\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "prods_to_plot = [1, 3, 4, 10]\n",
    "store_idx_to_plot = [0, 7]\n",
    "\n",
    "fig, ax = plt.subplots(\n",
    "    len(prods_to_plot), len(store_idx_to_plot),\n",
    "    figsize=(10, 18)\n",
    ")\n",
    "\n",
    "for (pidx, p) in enumerate(prods_to_plot):\n",
    "    for (sidx, s) in enumerate(store_idx_to_plot):\n",
    "        idx = (pidx, sidx)\n",
    "\n",
    "        subdf = df.query(\"brand == @p & store_idx == @sidx\")\n",
    "        subdf_idx = subdf.index\n",
    "\n",
    "        for row in range(spp_fp_logmove.shape[0]):\n",
    "            ax[idx].scatter(\n",
    "                subdf[\"logprice\"],\n",
    "                spp_fp_logmove[row, subdf_idx],\n",
    "                color=\"g\", alpha=0.01, s=35\n",
    "            )\n",
    "\n",
    "        ax[idx].scatter(\n",
    "            subdf[\"logprice\"], subdf[\"logmove\"],\n",
    "            color=\"k\", s=1\n",
    "        )\n",
    "\n",
    "        ax[idx].set_xlim(-4.0, -2.5)\n",
    "        ax[idx].set_ylim(3.5, 15)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Amnesia\n",
    "\n",
    "In the amnesia model, we will treat all products and stores as if they were the different and completely unrelated.\n",
    "\n",
    "The model can be written as:\n",
    "\n",
    "\\begin{align*}\n",
    "  \\log(q_{i, s, t}) &= \\alpha_{i, s} + \\eta_{i, s} \\log(p_{i, s, t}) + \\sigma_{i, s} \\varepsilon_{i, s, t} \\\\\n",
    "  \\alpha_{i, s} &\\sim N(1, 10) \\\\\n",
    "  \\eta_{i, s} &\\sim N(-1, 5) \\\\\n",
    "  \\sigma_{i, s} &\\sim \\text{HalfStudentT}(10, 5)\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Prep data\n",
    "log_q = df[\"logmove\"].to_numpy()\n",
    "log_p = df[\"logprice\"].to_numpy()\n",
    "\n",
    "product_idx = df[\"brand_idx\"].to_numpy()\n",
    "quality_idx = df[\"quality_idx\"].to_numpy()\n",
    "store_idx = df[\"store_idx\"].to_numpy()\n",
    "\n",
    "nproducts = df[\"brand_idx\"].nunique()\n",
    "nquality = df[\"quality_idx\"].nunique()\n",
    "nstores = df[\"store_idx\"].nunique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "m_np = pm.Model()\n",
    "\n",
    "with m_np:\n",
    "    # Data\n",
    "    _log_q = pm.Data(\"log_q\", log_q)\n",
    "    _log_p = pm.Data(\"log_p\", log_p)\n",
    "    _product_idx = pm.intX(pm.Data(\n",
    "        \"product_idx\", product_idx\n",
    "    ))\n",
    "    _store_idx = pm.intX(pm.Data(\n",
    "        \"store_idx\", store_idx\n",
    "    ))\n",
    "\n",
    "    # Priors\n",
    "    alpha = pm.Normal(\"alpha\", 1, 10, shape=(nstores, nproducts))\n",
    "    eta = pm.Normal(\"eta\", -1, 5, shape=(nstores, nproducts))\n",
    "    sigma = pm.HalfStudentT(\"sigma\", nu=10, sigma=5, shape=(nstores, nproducts))\n",
    "\n",
    "    # Likelihood\n",
    "    ll = pm.Normal(\n",
    "        \"ll\",\n",
    "        alpha[(_store_idx, _product_idx)] + eta[(_store_idx, _product_idx)]*_log_p,\n",
    "        sigma[(_store_idx, _product_idx)],\n",
    "        observed=_log_q\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Sample from the posterior**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "with m_np:\n",
    "    traces_np = pm.sample(2000, tune=1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "with m_np:\n",
    "    az.plot_trace(traces_np, compact=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Diagnostics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with m_np:\n",
    "    ess = az.ess(traces_np, relative=True)\n",
    "    rhat = az.rhat(traces_np)\n",
    "\n",
    "print(\"Effective Sample Size (min across parameters)\")\n",
    "print(f\"\\talpha: {ess['alpha'].values.min()}\")\n",
    "print(f\"\\teta: {ess['eta'].values.min()}\")\n",
    "print(f\"\\tsigma: {ess['sigma'].values.min()}\")\n",
    "print(\"rhat (max across parameters)\")\n",
    "print(f\"\\talpha: {rhat['alpha'].values.max()}\")\n",
    "print(f\"\\teta: {rhat['eta'].values.max()}\")\n",
    "print(f\"\\tsigma: {rhat['sigma'].values.max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Sampling posterior predictive**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "with m_np:\n",
    "    spp_np = pm.sample_posterior_predictive(traces_np, 500)\n",
    "\n",
    "spp_np_logmove = spp_np[\"ll\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "prods_to_plot = [1, 3, 4, 10]\n",
    "store_idx_to_plot = [0, 7]\n",
    "\n",
    "fig, ax = plt.subplots(\n",
    "    len(prods_to_plot), len(store_idx_to_plot),\n",
    "    figsize=(10, 18)\n",
    ")\n",
    "\n",
    "for (pidx, p) in enumerate(prods_to_plot):\n",
    "    for (sidx, s) in enumerate(store_idx_to_plot):\n",
    "        idx = (pidx, sidx)\n",
    "\n",
    "        subdf = df.query(\"brand == @p & store_idx == @sidx\")\n",
    "        subdf_idx = subdf.index\n",
    "\n",
    "        for row in range(spp_fp_logmove.shape[0]):\n",
    "            ax[idx].scatter(\n",
    "                subdf[\"logprice\"],\n",
    "                spp_np_logmove[row, subdf_idx],\n",
    "                color=\"g\", alpha=0.01, s=35\n",
    "            )\n",
    "\n",
    "        ax[idx].scatter(\n",
    "            subdf[\"logprice\"], subdf[\"logmove\"],\n",
    "            color=\"k\", s=1\n",
    "        )\n",
    "\n",
    "        ax[idx].set_xlim(-4.0, -2.5)\n",
    "        ax[idx].set_ylim(3.5, 15)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Pooled by product\n",
    "\n",
    "This will be our first hierarchical model. We will assume that the elasticity differs by product but is the same across all stores.\n",
    "\n",
    "The model can be written as:\n",
    "\n",
    "\\begin{align*}\n",
    "  \\log(q_{i, s, t}) &= \\alpha_{i, s} + \\eta_{i} \\log(p_{i, s, t}) + \\sigma_{i} \\varepsilon_{i, s, t} \\\\\n",
    "  \\alpha_{i, s} &\\sim N(1, 10) \\\\\n",
    "  \\eta_{i} &\\sim N(\\bar{\\eta}, \\sigma^\\eta) \\\\\n",
    "  \\sigma_{i} &\\sim \\text{HalfStudentT}(10, 5) \\\\\n",
    "  \\bar{\\eta} &\\sim N(-1, 5) \\\\\n",
    "  \\sigma^\\eta &\\sim \\text{HalfStudentT}(5, 2)\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Prep data\n",
    "log_q = df[\"logmove\"].to_numpy()\n",
    "log_p = df[\"logprice\"].to_numpy()\n",
    "\n",
    "product_idx = df[\"brand_idx\"].to_numpy()\n",
    "store_idx = df[\"store_idx\"].to_numpy()\n",
    "\n",
    "nproducts = df[\"brand_idx\"].unique().shape[0]\n",
    "nstores = df[\"store_idx\"].unique().shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "m_hs = pm.Model()\n",
    "\n",
    "with m_hs:\n",
    "    # Data\n",
    "    _log_q = pm.Data(\"log_q\", log_q)\n",
    "    _log_p = pm.Data(\"log_p\", log_p)\n",
    "    _product_idx = pm.intX(pm.Data(\n",
    "        \"product_idx\", product_idx\n",
    "    ))\n",
    "    _store_idx = pm.intX(pm.Data(\n",
    "        \"store_idx\", store_idx\n",
    "    ))\n",
    "\n",
    "    # Hyper priors\n",
    "    eta_bar = pm.Normal(\"eta_bar\", -1, 5)\n",
    "    sigma_eta = pm.HalfStudentT(\"sigma_eta\", nu=5, sigma=3)\n",
    "\n",
    "    # Priors\n",
    "    alpha = pm.Normal(\"alpha\", 1, 10, shape=(nstores, nproducts))\n",
    "    eta = pm.Normal(\"eta\", eta_bar, sigma_eta, shape=nproducts)\n",
    "    sigma = pm.HalfStudentT(\"sigma\", nu=10, sigma=5, shape=nproducts)\n",
    "\n",
    "    # Likelihood\n",
    "    ll = pm.Normal(\n",
    "        \"ll\",\n",
    "        alpha[(_store_idx, _product_idx)] + eta[_product_idx]*_log_p,\n",
    "        sigma[_product_idx],\n",
    "        observed=_log_q\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Sample from the posterior**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "with m_hs:\n",
    "    traces_hs = pm.sample(2000, tune=1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "with m_hs:\n",
    "    az.plot_trace(\n",
    "        traces_hs, var_names=[\"alpha\", \"sigma\", \"eta\", \"eta_bar\", \"sigma_eta\"],\n",
    "        compact=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Diagnostics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with m_hs:\n",
    "    ess = az.ess(traces_hs, relative=True)\n",
    "    rhat = az.rhat(traces_hs)\n",
    "\n",
    "print(\"Effective Sample Size (min across parameters)\")\n",
    "print(f\"\\talpha: {ess['alpha'].values.min()}\")\n",
    "print(f\"\\teta: {ess['eta'].values.min()}\")\n",
    "print(f\"\\tsigma: {ess['sigma'].values.min()}\")\n",
    "print(\"rhat (max across parameters)\")\n",
    "print(f\"\\talpha: {rhat['alpha'].values.max()}\")\n",
    "print(f\"\\teta: {rhat['eta'].values.max()}\")\n",
    "print(f\"\\tsigma: {rhat['sigma'].values.max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Sampling posterior predictive**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "with m_hs:\n",
    "    spp_hs = pm.sample_posterior_predictive(traces_hs, 250)\n",
    "\n",
    "spp_hs_logmove = spp_hs[\"ll\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prods_to_keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "prods_to_plot = [1, 3, 4, 10]\n",
    "store_idx_to_plot = [0, 7]\n",
    "\n",
    "fig, ax = plt.subplots(\n",
    "    len(prods_to_plot), len(store_idx_to_plot),\n",
    "    figsize=(10, 18)\n",
    ")\n",
    "\n",
    "for (pidx, p) in enumerate(prods_to_plot):\n",
    "    for (sidx, s) in enumerate(store_idx_to_plot):\n",
    "        idx = (pidx, sidx)\n",
    "\n",
    "        subdf = df.query(\"brand == @p & store_idx == @sidx\")\n",
    "        subdf_idx = subdf.index\n",
    "\n",
    "        for row in range(spp_hs_logmove.shape[0]):\n",
    "            ax[idx].scatter(\n",
    "                subdf[\"logprice\"],\n",
    "                spp_np_logmove[row, subdf_idx],\n",
    "                color=\"g\", alpha=0.01, s=35\n",
    "            )\n",
    "\n",
    "        ax[idx].scatter(\n",
    "            subdf[\"logprice\"], subdf[\"logmove\"],\n",
    "            color=\"k\", s=1\n",
    "        )\n",
    "\n",
    "        ax[idx].set_xlim(-4.0, -2.5)\n",
    "        ax[idx].set_ylim(3.5, 15)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Multi-layered hierarchical\n",
    "\n",
    "This will be our first multi-layered hierarchical model.\n",
    "\n",
    "We will allow elasticity to differ by both product and store, but we will do it in a particular way -- We will assume that a product's elasticity for each store is drawn from a product specific distribution and that the product specific distribution comes from a distribution shared by products.\n",
    "\n",
    "The model can be written as:\n",
    "\n",
    "\\begin{align*}\n",
    "  \\log(q_{i, s, t}) &= \\alpha_{i, s} + \\eta_{i} \\log(p_{i, s, t}) + \\sigma_{i} \\varepsilon \\\\\n",
    "  \\alpha_{i, s} &\\sim N(1, 10) \\\\\n",
    "  \\sigma_{i} &\\sim \\text{HalfStudentT}(10, 5) \\\\\n",
    "  \\eta_{i, s} &\\sim N(\\bar{\\eta}_i, \\sigma^\\eta_i) \\\\\n",
    "  \\bar{\\eta}_i &\\sim N(\\tilde{\\eta}, \\tilde{\\sigma}) \\\\\n",
    "  \\tilde{\\eta} &\\sim N(-1, 3) \\\\\n",
    "  \\tilde{\\sigma} &\\sim \\text{HalfStudentT}(5, 3)\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Prep data\n",
    "log_q = df[\"logmove\"].to_numpy()\n",
    "log_p = df[\"logprice\"].to_numpy()\n",
    "\n",
    "product_idx = df[\"brand_idx\"].to_numpy()\n",
    "store_idx = df[\"store_idx\"].to_numpy()\n",
    "\n",
    "nproducts = df[\"brand_idx\"].unique().shape[0]\n",
    "nstores = df[\"store_idx\"].unique().shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_mlhs = pm.Model()\n",
    "\n",
    "with m_mlhs:\n",
    "    # Data\n",
    "    _log_q = pm.Data(\"log_q\", log_q)\n",
    "    _log_p = pm.Data(\"log_p\", log_p)\n",
    "    _product_idx = pm.intX(pm.Data(\n",
    "        \"product_idx\", product_idx\n",
    "    ))\n",
    "    _store_idx = pm.intX(pm.Data(\n",
    "        \"store_idx\", store_idx\n",
    "    ))\n",
    "\n",
    "    # Hyperhyper priors\n",
    "    eta_tilde = pm.Normal(\"eta_tilde\", -1, 5)\n",
    "    sigma_tilde = pm.HalfStudentT(\"sigma_tilde\", nu=5, sigma=3)\n",
    "\n",
    "    # Hyperpriors\n",
    "    eb_offset = pm.Normal(\"eb_offset\", 0, 1, shape=nproducts)\n",
    "    eta_bar = pm.Deterministic(\n",
    "        \"eta_bar\", eta_tilde + sigma_tilde*eb_offset\n",
    "    )\n",
    "    sigma_eta = pm.HalfStudentT(\"sigma_eta\", nu=5, sigma=3, shape=nproducts)\n",
    "\n",
    "    # Priors\n",
    "    alpha = pm.Normal(\"alpha\", 1, 10, shape=(nstores, nproducts))\n",
    "    eta_offset = pm.Normal(\"eta_offset\", 0, 1, shape=(nstores, nproducts))\n",
    "    eta = pm.Deterministic(\n",
    "        \"eta\", eta_bar[None, :] + eta_offset*sigma_eta[None, :]\n",
    "    )\n",
    "    sigma = pm.HalfStudentT(\"sigma\", nu=10, sigma=5, shape=nproducts)\n",
    "\n",
    "    # Likelihood\n",
    "    ll = pm.Normal(\n",
    "        \"ll\",\n",
    "        alpha[(_store_idx, _product_idx)] + eta[(_store_idx, _product_idx)]*_log_p,\n",
    "        sigma[_product_idx],\n",
    "        observed=_log_q\n",
    "    )\n",
    "\n",
    "# Centered model\n",
    "# m_mlhs = pm.Model()\n",
    "\n",
    "# with m_:\n",
    "#     # Data\n",
    "#     _log_q = pm.Data(\"log_q\", log_q)\n",
    "#     _log_p = pm.Data(\"log_p\", log_p)\n",
    "#     _product_idx = pm.intX(pm.Data(\n",
    "#         \"product_idx\", product_idx\n",
    "#     ))\n",
    "#     _store_idx = pm.intX(pm.Data(\n",
    "#         \"store_idx\", store_idx\n",
    "#     ))\n",
    "\n",
    "#     # Hyperhyper priors\n",
    "#     eta_tilde = pm.Normal(\"eta_tilde\", -1, 5)\n",
    "#     sigma_tilde = pm.HalfStudentT(\"sigma_tilde\", nu=5, sigma=3)\n",
    "\n",
    "#     # Hyperpriors\n",
    "#     eta_bar = pm.Normal(\"eta_bar\", eta_tilde, sigma_tilde, shape=nproducts)\n",
    "#     sigma_eta = pm.HalfStudentT(\"sigma_eta\", nu=5, sigma=3, shape=nproducts)\n",
    "\n",
    "#     # Priors\n",
    "#     alpha = pm.Normal(\"alpha\", 1, 10, shape=(nstores, nproducts))\n",
    "#     eta = pm.Normal(\"eta\", eta_bar[None, :], sigma_eta[None, :], shape=(nstores, nproducts))\n",
    "#     sigma = pm.HalfStudentT(\"sigma\", nu=10, sigma=5, shape=nproducts)\n",
    "\n",
    "#     # Likelihood\n",
    "#     ll = pm.Normal(\n",
    "#         \"ll\",\n",
    "#         alpha[(_store_idx, _product_idx)] + eta[(_store_idx, _product_idx)]*_log_p,\n",
    "#         sigma[_product_idx],\n",
    "#         observed=_log_q\n",
    "#     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Why did we build the model this way? We think the elasticity is more likely to be similar within a product than within a store so we want the distribution that the elasticities are being drawn from to be related to product rather than store.\n",
    "\n",
    "This could have been achieved in other ways and it would also be reasonable to take a different view than we took"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Sample from the posterior**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "with m_mlhs:\n",
    "    traces_mlhs = pm.sample(1500, tune=1000, target_accept=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "with m_mlhs:\n",
    "    az.plot_trace(\n",
    "        traces_mlhs, var_names=[\n",
    "            \"alpha\", \"sigma\", \"eta\", \"eta_bar\", \"sigma_eta\",\n",
    "            \"eta_tilde\", \"sigma_tilde\"\n",
    "        ],\n",
    "        compact=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Diagnostics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "with m_mlhs:\n",
    "    ess = az.ess(traces_mlhs, relative=True)\n",
    "    rhat = az.rhat(traces_mlhs)\n",
    "\n",
    "print(\"Effective Sample Size (min across parameters)\")\n",
    "print(f\"\\talpha: {ess['alpha'].values.min()}\")\n",
    "print(f\"\\teta: {ess['eta'].values.min()}\")\n",
    "print(f\"\\tsigma: {ess['sigma'].values.min()}\")\n",
    "print(\"rhat (max across parameters)\")\n",
    "print(f\"\\talpha: {rhat['alpha'].values.max()}\")\n",
    "print(f\"\\teta: {rhat['eta'].values.max()}\")\n",
    "print(f\"\\tsigma: {rhat['sigma'].values.max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Sampling posterior predictive**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "with m_mlhs:\n",
    "    spp_mlhs = pm.sample_posterior_predictive(traces_mlhs, 250)\n",
    "\n",
    "spp_mlhs_logmove = spp_mlhs[\"ll\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "prods_to_plot = [1, 3, 4, 10]\n",
    "store_idx_to_plot = [0, 7]\n",
    "\n",
    "fig, ax = plt.subplots(\n",
    "    len(prods_to_plot), len(store_idx_to_plot),\n",
    "    figsize=(10, 18)\n",
    ")\n",
    "\n",
    "for (pidx, p) in enumerate(prods_to_plot):\n",
    "    for (sidx, s) in enumerate(store_idx_to_plot):\n",
    "        idx = (pidx, sidx)\n",
    "\n",
    "        subdf = df.query(\"brand == @p & store_idx == @sidx\")\n",
    "        subdf_idx = subdf.index\n",
    "\n",
    "        for row in range(spp_mlhs_logmove.shape[0]):\n",
    "            ax[idx].scatter(\n",
    "                subdf[\"logprice\"],\n",
    "                spp_mlhs_logmove[row, subdf_idx],\n",
    "                color=\"g\", alpha=0.01, s=35\n",
    "            )\n",
    "\n",
    "        ax[idx].scatter(\n",
    "            subdf[\"logprice\"], subdf[\"logmove\"],\n",
    "            color=\"k\", s=1\n",
    "        )\n",
    "\n",
    "        ax[idx].set_xlim(-4.0, -2.5)\n",
    "        ax[idx].set_ylim(3.5, 15)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Centered vs Non-Centered Hierarchical Models\n",
    "\n",
    "We won't be able to talk about this as much as we would like, but we wanted to at least raise this issue because it is a serious issue that can arise when working with hierarchical models\n",
    "\n",
    "If you'd like more information about this issue, we recommend reading: [1](https://twiecki.io/blog/2017/02/08/bayesian-hierchical-non-centered/), [2](https://mc-stan.org/docs/2_18/stan-users-guide/reparameterization-section.html), [3](https://mc-stan.org/users/documentation/case-studies/divergences_and_bias.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### The issue\n",
    "\n",
    "Let's focus on a somewhat generic hierarchical model where,\n",
    "\n",
    "\\begin{align*}\n",
    "  \\beta_i &\\sim N(\\bar{\\beta}, \\bar{\\sigma}) \\\\\n",
    "  \\bar{\\beta} &\\sim N(\\mu, \\sigma) \\\\\n",
    "  \\bar{\\sigma} &\\sim \\text{HalfStudentT}(\\nu, \\tau)\n",
    "\\end{align*}\n",
    "\n",
    "We would refer to this model as the _centered model_, however, it could be written another way as well\n",
    "\n",
    "\\begin{align*}\n",
    "  \\beta_i &= \\bar{\\beta} + \\bar{\\sigma} \\beta_\\text{offset} \\\\\n",
    "  \\beta_\\text{offset} &\\sim N(0, 1) \\\\\n",
    "  \\bar{\\beta} &\\sim N(\\mu, \\sigma) \\\\\n",
    "  \\bar{\\sigma} &\\sim \\text{HalfStudentT}(\\nu, \\tau)\n",
    "\\end{align*}\n",
    "\n",
    "This is what we refer to as the _non-centered model_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We use an image from the blogpost by Thomas Wiecki that we linked to above to show why the non-centered model can be helpful\n",
    "\n",
    "* Non-centered weakens the relationship between $\\hat{\\beta}$ and $\\hat{\\sigma}$ by introducing $\\beta_{\\text{offset}}$\n",
    "* This then allows the Markov chain to more easily enter the \"funnel\" portion of the posterior\n",
    "\n",
    "![](funnel.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Why does this matter?**\n",
    "\n",
    "The centered version of the model ends up overestimating the standard deviation which means that you believe that you're more capable of separating the grouped coefficients than you actually are\n",
    "\n",
    "![](sigma_centered_vs_noncentered.png)\n",
    "\n",
    "Again, this graph also comes from the [Thomas Wiecki post](https://twiecki.io/blog/2017/02/08/bayesian-hierchical-non-centered/)... I highly recommend reading it if this interests you."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
