{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Convex Optimization: Quadratic Programming\n",
    "\n",
    "**Prerequisites**\n",
    "\n",
    "- Linear Algebra\n",
    "- Calculus\n",
    "- Convex Optimization: Theoretical Foundations\n",
    "- Convex Optimization: Linear Programs\n",
    "\n",
    "**Outcomes**\n",
    "\n",
    "- Know the general structure of quadratic programs\n",
    "- Map linear least squares into a quadratic program\n",
    "- Solve constrained linear least squares via quadratic programming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment to install cvxpy if necessary\n",
    "# %pip install --user cvxpy\n",
    "\n",
    "import cvxpy as cp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use(\"ggplot\")\n",
    "pd.set_option('display.float_format', lambda x: '%.4f' % x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Quadratic Programming\n",
    "\n",
    "Recall the general form of a linear program:\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\min_x \\ & c^T x \\\\\n",
    "& Ax \\le b,\\\\\n",
    "& x \\ge 0\n",
    "\\end{align*}$$\n",
    "\n",
    "Notice that both the objective function and the constraints are linear in the choice variable $x$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### General Form\n",
    "\n",
    "An extension to this framework is to allow the objective function to be quadratic in $x$\n",
    "\n",
    "An optimization problem of this is called quadratic programming\n",
    "\n",
    "The general form of a quadratic program is\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\min_x \\ & \\frac{1}{2} x^T P x +  q^T x + r\\\\\n",
    "& Gx \\le h,\\\\\n",
    "& Ax = b.\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Geometry\n",
    "\n",
    "The feasible space for a quadratic program is defined by the constraints\n",
    "\n",
    "Because these constraints are all linear in $x$ (as in a linear program), the feasible space is a polygon (as in LP)\n",
    "\n",
    "![QPfeasibility](qp_feasibility.png)\n",
    "\n",
    "> Reference: Boyd et. al. 2004"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## QP with cvxpy\n",
    "\n",
    "We can use the `cvxpy` library to represent and solve quadratic programs\n",
    "\n",
    "As an example suppose we need to minimize the function\n",
    "\n",
    "$$g(x, y, z) = \\frac{3}{2} x^2 + 2 xy + xz + 2y^2 + 2yz + \\frac{3}{2}z^2+ 3x + z$$\n",
    "\n",
    "Subject to the constraint that $x$, $y$, and $z$ are all at least -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "We need to formulate the matrix $P$, vector $q$, and scalar $r$ from the general form of the QP\n",
    "\n",
    "We can do this by reading off coefficients:\n",
    "\n",
    "\\begin{align*}\n",
    "P &= \\left[\\begin{matrix}3 & 2 & 1\\\\2 & 4 & 2\\\\1 & 2 & 3\\end{matrix}\\right]\\\\\n",
    "q &= \\left[\\begin{matrix}3\\\\0\\\\1\\end{matrix}\\right] \\\\\n",
    "r &= 0\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can represent $P$ and $q$ as numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qp_example_arrays():\n",
    "    P = np.array([[3, 2, 1],\n",
    "                  [2, 4, 2],\n",
    "                  [1, 2, 3]])\n",
    "    q = np.array([3, 0, 1])\n",
    "    r = 0\n",
    "    return P, q, r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "And now let's write a function that can use these inputs to solve our QP with cvxpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_qp1(P, q, r):\n",
    "    N = len(q)\n",
    "    assert P.shape[0] == P.shape[1] == N\n",
    "    \n",
    "    x = cp.Variable(N)\n",
    "    obj = cp.Minimize(1/2*cp.quad_form(x, P) + q.T@x + r)\n",
    "    prob = cp.Problem(\n",
    "        obj,       # objective\n",
    "        [x >= -1],  # list of constraints\n",
    "    )\n",
    "    \n",
    "    ans = prob.solve()\n",
    "    \n",
    "    return x.value, prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob1.constraints[0].dual_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "opt_x, prob1 = solve_qp1(*qp_example_arrays())\n",
    "opt_x, prob1.value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Exercise**\n",
    "\n",
    "In the code cell below you will find the outline for a Python class called `QP`\n",
    "\n",
    "Your task is to implement the functions of `QP` such that the class can \n",
    "\n",
    "- Take in variables representing $P$, $q$, $r$, $G$, $h$, $A$, and $b$ from the general form of the quadratic program\n",
    "- Formulate the general form quadratic program using `cvxpy`\n",
    "- Solve the quadratic program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QP:\n",
    "    \"\"\"\n",
    "    Formulate and solve a general form quadratic program using cvxpy\n",
    "    \n",
    "    The quadratic program has the following representation\n",
    "    \n",
    "    min_x 1/2 x' P x + q'x + r\n",
    "    s.t. G x <= h\n",
    "         A x  = b\n",
    "    \"\"\"\n",
    "    def __init__(self, P, q, r, G, h, A, b):\n",
    "        pass\n",
    "    \n",
    "        # change the code below to create a cvxpy Variable\n",
    "        self.x = None\n",
    "        \n",
    "        # formulate the QP here\n",
    "        self.prob = None\n",
    "    \n",
    "    def solve(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Special Case: Linear Regression\n",
    "\n",
    "Now we will study the linear regression problem, from a convex optimization perspective\n",
    "\n",
    "The linear regression problem takes as an input $(x, y)$ pairs of observed data (where $x$ and $y$ may be vector valued) and a proposed manifold of models of the form\n",
    "\n",
    "$$y = x \\beta + \\epsilon, \\quad \\epsilon \\sim N\\left(0, \\sigma^2\\right)$$\n",
    "\n",
    "The task of linear regression is to select the parameter vector $\\beta$ that maximizes the likelihood, given the data $(x, y)$\n",
    "\n",
    "It can be shown that maximizing the likelihood is equivalent to minimizing the sum of squared residuals, which is defined as\n",
    "\n",
    "$$r \\equiv \\sum (x \\beta - y)^2 = ||x\\beta - y||_2^2$$\n",
    "\n",
    "For this reason, linear regression is also called least squares (LS) or least squares regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Analytical Solution\n",
    "\n",
    "It turns out that the unconstrained LS problem can be solved analytically\n",
    "\n",
    "We'll work that out now, starting by manipulating the squared residual expression\n",
    "\n",
    "\\begin{align*}\n",
    "||x\\beta - y||_2^2 &= \\left [(x \\beta - y)^T(x \\beta - y) \\right] \\\\\n",
    "&= \\beta^T x^T x \\beta - \\beta^T x^T y - y^T x \\beta + y^T y \\\\ \n",
    "&= \\beta^T x^T x \\beta - 2 y^T x \\beta + y^T y\n",
    "\\end{align*}\n",
    "\n",
    "Now we differentiate with respect to $\\beta$ and set equal to 0:\n",
    "\n",
    "\\begin{align*}\n",
    "0 &= 2 x^T x \\beta - 2 x^T y &\\Longrightarrow \\\\\n",
    "x^Tx \\beta &= x^T y &\\Longrightarrow \\\\\n",
    "\\beta &= (x^T x)^{-1} x^T y\n",
    "\\end{align*}\n",
    "\n",
    "We will leverage the fact that we know the analytical solution below..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can also express the LS problem as a quadratic program\n",
    "\n",
    "There are (for now) no constraints, so we are left with expressing the objective function in the form of a general form QP\n",
    "\n",
    "Above we showed that \n",
    "\n",
    "$$||\\beta x - y||_2^2 \\equiv \\beta^T x^T x \\beta - 2 y^T x \\beta + y^T y,$$\n",
    "\n",
    "Which means we have a quadratic program\n",
    "\n",
    "$$\\min_{\\beta} (1/2) \\beta^T P \\beta + q^T \\beta + r$$\n",
    "\n",
    "with \n",
    "\n",
    "\\begin{align*}\n",
    "P &= 2 x^T x \\\\\n",
    "q &= -2 y^T x \\\\\n",
    "r &= y^T y\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Example: house prices\n",
    "\n",
    "We'll do an example using data from house prices in King County Washington (near the city Seattle) from May 2014 to May 2015\n",
    "\n",
    "Let's first load the data and take a look\n",
    "\n",
    "> Note: The data comes from [Kaggle](https://www.kaggle.com/harlfoxem/housesalesprediction) . Variable definitions and additional documentation are available at that link."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "url = \"https://datascience.quantecon.org/assets/data/kc_house_data.csv\"\n",
    "df = pd.read_csv(url)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# construct \"x\" 1/2 of observed data\n",
    "df[\"log_sqft_living\"] = np.log(df[\"sqft_living\"])\n",
    "x_cols = [\n",
    "    'bedrooms',\n",
    "    'bathrooms',\n",
    "    'floors',\n",
    "    'waterfront',\n",
    "    'view',\n",
    "    'condition',\n",
    "    \"log_sqft_living\"\n",
    "]\n",
    "\n",
    "x = df.loc[:, x_cols].copy().astype(float)\n",
    "x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# notice the log here!\n",
    "y = np.log(df[\"price\"])\n",
    "df[\"log_price\"] = y\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "df.plot.scatter(x=\"log_sqft_living\" , y=\"log_price\", alpha=0.35, s=1.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We'll first compute the analytical solution:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_arr = x.to_numpy()\n",
    "y_arr = y.to_numpy()\n",
    "beta_hat = np.linalg.inv(x_arr.T @ x_arr) @ x_arr.T @ y_arr\n",
    "pd.Series(beta_hat, index=list(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Now, let's solve this as a quadratic program using cvxpy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def houses_qp(x, y):\n",
    "    P = 2 * x.T @ x\n",
    "    q = -2 * y.T @  x\n",
    "    r = float(y @ y)\n",
    "    \n",
    "    beta = cp.Variable(len(q))\n",
    "    obj = cp.Minimize(1/2*cp.quad_form(beta, P) + q.T@beta + r)\n",
    "    prob = cp.Problem(\n",
    "        obj,       # objective\n",
    "        [],        # list of constraints\n",
    "    )\n",
    "    ans = prob.solve()\n",
    "    \n",
    "    return beta.value, prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "beta_qp, prob_houses = houses_qp(x_arr, y_arr)\n",
    "betas = pd.DataFrame(dict(analytical=beta_hat, qp=beta_qp), index=list(x))\n",
    "betas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the beta computed analytically matches very closely to the beta computed via quadratic programming\n",
    "\n",
    "If we look at the value of the objective function (sum of squared residuals or SSR) at the optimal values we see that they are almost identical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "prob_houses.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "((x @ betas).sub(y, axis=0)**2).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Why?\n",
    "\n",
    "If we end up with a slightly worse parameter vector beta (in the sense that the SSR is larger), why solve as quadratic program?\n",
    "\n",
    "**constraints** and **regularization**\n",
    "\n",
    "The LS problem is only analytically tractable if there are no constraints and the objective function is strictly the SSR\n",
    "\n",
    "If we would like to add constraints on the value of our parameter vector $\\beta$ we must use a constrained least squares routine... or quadratic programming!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Extension: Constrained Regression\n",
    "\n",
    "Suppose you work for a financial institution\n",
    "\n",
    "Your firm has the capacity to invest in $N$ different assets (numbered 1 to $N$)\n",
    "\n",
    "One day your boss comes to you and tells you to invest in asset $0$\n",
    "\n",
    "However, due to regulatory constraints you are not allowed to open a position in asset $0$\n",
    "\n",
    "What you decide to do is construct a portfolio over assets 1 to $N$ that approximate as closely as possible exposure to asset $0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Knowing quadratic programming and least squares regression you set up the following least squares regression problem:\n",
    "\n",
    "\\begin{align*}\n",
    "y_0 &= \\sum_{i=1}^N \\beta_i y_i + \\epsilon \\\\\n",
    "&= y \\beta + \\epsilon\n",
    "\\end{align*}\n",
    "\n",
    "Where you have two sets of constraints:\n",
    "\n",
    "1. $\\beta_i >= 0 \\forall i$\n",
    "2. $\\sum_i \\beta_i = 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The associated QP is\n",
    "\n",
    "\\begin{align*}\n",
    "\\min_{\\beta} \\quad &|| \\beta y - y_0 ||_2^2 \\\\\n",
    "s.t. \\quad & \\mathbf{1}^T\\beta = 1 \\\\\n",
    " & \\beta \\ge 0\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "The vector $\\beta$ can then be used as portfolio weights for assets 1 to $N$ such that the expected behavior of the portfolio matches the behavior of asset 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Example: US Equities\n",
    "\n",
    "Suppose that asset 0 is Apple Inc. stock (ticker AAPL)\n",
    "\n",
    "We have 5 other equities: Walt Disney Co (DIS), Home Depot (HD), McDonalds (MCD), Microsoft (MSFT), Nike (NKE)\n",
    "\n",
    "Our goal is to construct a synthetic exposure to AAPL using the other 6 equities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"equities.csv\", parse_dates=[\"Date\"], index_col=[\"Date\"])\n",
    "df.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "y0_df = df[\"AAPL\"]\n",
    "y_df = df.drop(\"AAPL\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def aapl_exposure(y_df, y0_df):\n",
    "    y0 = y0_df.to_numpy()\n",
    "    y = y_df.to_numpy()\n",
    "    P = 2 * y.T @ y\n",
    "    q = -2 * y0.T @  y\n",
    "    r = float(y0 @ y0)\n",
    "    \n",
    "    beta = cp.Variable(len(q))\n",
    "    obj = cp.Minimize(1/2*cp.quad_form(beta, P) + q.T@beta + r)\n",
    "    prob = cp.Problem(\n",
    "        obj,                          # objective\n",
    "        [beta >= 0, sum(beta) == 1],  # list of constraints\n",
    "    )\n",
    "    prob.solve()\n",
    "    \n",
    "    return pd.Series(beta.value, index=list(y_df)), prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "beta_aapl, prob_aapl = aapl_exposure(y_df, y0_df)\n",
    "beta_aapl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_aapl.constraints[0].dual_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_aapl.constraints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The synthetic asset is composed of about 1/4 MSFT and 3/4 NKE\n",
    "\n",
    "Let's see what the daily returns of AAPL vs our portfolio look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "(y_df @ beta_aapl).pct_change().rename(\"approx\").plot(ax=ax, alpha=0.7)\n",
    "y0_df.pct_change().plot(ax=ax, alpha=0.7)\n",
    "ax.legend(loc=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Extension: Regularization\n",
    "\n",
    "Another extension we can make to the standard LS problem using QP is regularization\n",
    "\n",
    "Regularization can be loosely thought of as any method that seeks to \"tame\" the value of parameters\n",
    "\n",
    "The goal behind regularization is to avoid extreme parameter values\n",
    "\n",
    "In predictive settings the rationale behind that is to avoid having a model or optimization routine focus too strongly on quirks that appear in training data that do not correspond to generic patterns in the underlying population"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "One very common form of regularization is called L2-regularization or Tychonov regularization\n",
    "\n",
    "This is applied by adding the following term to the objective function\n",
    "\n",
    "$$\\lambda || x ||_2^2,$$\n",
    "\n",
    "where $\\lambda$ is a constant that governs the strength of the regularization\n",
    "\n",
    "When this term is added to the objective function, the values of $x$ are compressed towards zero (relative to the solution where this term is not added)\n",
    "\n",
    "As $\\lambda$ is increased, this compression is stronger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's implement L2-regularization with cvxpy using our housing data example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def houses_qp_l2(x, y, lam):   \n",
    "    beta = cp.Variable(x.shape[1])\n",
    "    obj = cp.Minimize(\n",
    "        cp.sum_squares(x @ beta - y) + lam * cp.sum_squares(beta)\n",
    "    )\n",
    "    prob = cp.Problem(\n",
    "        obj,       # objective\n",
    "        [],        # list of constraints\n",
    "    )\n",
    "    ans = prob.solve()\n",
    "    \n",
    "    return beta.value, prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "for lam in [100, 1000, 5000, 10000]:\n",
    "    betas[f\"qp_l2_{lam}\"] = houses_qp_l2(x_arr, y_arr, lam)[0]\n",
    "    \n",
    "betas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1.78**2 + 0.16 ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1.4043 ** 2 + 0.4661 ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "((x@betas).sub(y, axis=0)**2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(betas**2).sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example: Huber Regression\n",
    "\n",
    "We'll now work through an example of something called Huber Regression\n",
    "\n",
    "This example was originally created by the cvxpy team\n",
    "\n",
    "We accessed the example at the following GitHub repository and have repeated it here in this notebook almost verbatim: https://github.com/cvxgrp/cvx_short_course/blob/master/applications/huber_regression.ipynb\n",
    "\n",
    "Credit (and gratitude!) goes to the original authors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The function $\\phi(u;M)$ below is called the Huber function\n",
    "\n",
    "$$\n",
    "\\phi(u;M) = \\left\\{ \\begin{array}{ll} u^2 & |u|\\leq M\\\\\n",
    "2M |u| - M^2 & |u|>M\n",
    "\\end{array}\\right.\n",
    "$$\n",
    "\n",
    "Relative to the squared function, the Huber function is more permissive of values of $u$ that are greater in absolute value than $M$\n",
    "\n",
    "In the cells below we plot this function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class Huber:\n",
    "    def __init__(self, M):\n",
    "        self.M = M\n",
    "    \n",
    "    def __call__(self, u):\n",
    "        out = np.zeros(len(u))\n",
    "        small = np.abs(u) <= self.M\n",
    "        out[small] = u[small]**2\n",
    "        out[~small] = 2*self.M*abs(u[~small]) - self.M**2\n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "phi = Huber(1)\n",
    "u = np.linspace(-4, 4, 100)\n",
    "ax.plot(u, phi(u), label=\"Huber\")\n",
    "ax.plot(u, u**2, label=\"L2\")\n",
    "ax.legend(loc=0);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Example\n",
    "\n",
    "In the following code we do a numerical example of Huber regression.\n",
    "We generate $m=450$ measurements with $n=300$ regressors\n",
    "\n",
    "We randomly choose $\\beta^\\mathrm{true}$ and $x_i \\sim \\mathcal N(0,I)$\n",
    "\n",
    "We set $y_i = (\\beta^\\mathrm{true})^Tx_i + \\epsilon_i$, where $\\epsilon_i \\sim\n",
    "\\mathcal N(0,1)$\n",
    "\n",
    "Then with probability $p$ we replace $y_i$ with $-y_i$\n",
    "\n",
    "The data has fraction $p$ of (non-obvious) wrong measurements\n",
    "\n",
    "The distribution of \"good\" and \"bad\" $y_i$ are the same"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Our goal is to recover $\\beta^\\mathrm{true} \\in {\\bf R}^n$ from the measurements $y\\in {\\bf R}^m$\n",
    "\n",
    "We compare three approaches: \n",
    "\n",
    "1. standard regression\n",
    "2. Huber regression\n",
    "3. \"prescient\" regression, where we know which measurements had their sign flipped\n",
    "\n",
    "We generate $50$ problem instances, with $p$ varying from $0$ to $0.15$, and plot the relative error in reconstructing $\\beta^\\mathrm{true}$ for the three approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Notice that in the range $p \\in [0,0.08]$, Huber regression matches prescient regression\n",
    "\n",
    "Standard regression, by contrast, fails even for very small $p$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Generate data for Huber regression.\n",
    "import numpy as np\n",
    "np.random.seed(1)\n",
    "n = 300\n",
    "SAMPLES = int(1.5*n)\n",
    "beta_true = 5*np.random.normal(size=(n,1))\n",
    "X = np.random.randn(n, SAMPLES)\n",
    "Y = np.zeros((SAMPLES,1))\n",
    "v = np.random.normal(size=(SAMPLES,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Generate data for different values of p.\n",
    "# Solve the resulting problems.\n",
    "# WARNING this script takes a few minutes to run.\n",
    "import cvxpy as cp\n",
    "\n",
    "TESTS = 50\n",
    "lsq_data = np.zeros(TESTS)\n",
    "huber_data = np.zeros(TESTS)\n",
    "prescient_data = np.zeros(TESTS)\n",
    "p_vals = np.linspace(0,0.15, num=TESTS)\n",
    "for idx, p in enumerate(p_vals):\n",
    "    # Generate the sign changes.\n",
    "    factor = 2*np.random.binomial(1, 1-p, size=(SAMPLES,1)) - 1\n",
    "    Y = factor*X.T.dot(beta_true) + v\n",
    "    \n",
    "    # Form and solve a standard regression problem.\n",
    "    beta = cp.Variable((n,1))\n",
    "    fit = cp.norm(beta - beta_true)/cp.norm(beta_true)\n",
    "    cost = cp.norm(X.T@beta - Y)\n",
    "    prob = cp.Problem(cp.Minimize(cost))\n",
    "    prob.solve()\n",
    "    lsq_data[idx] = fit.value\n",
    "    \n",
    "    # Form and solve a prescient regression problem,\n",
    "    # i.e., where the sign changes are known.\n",
    "    cost = cp.norm(cp.multiply(factor, X.T@beta) - Y)\n",
    "    cp.Problem(cp.Minimize(cost)).solve()\n",
    "    prescient_data[idx] = fit.value\n",
    "    \n",
    "    # Form and solve the Huber regression problem.\n",
    "    cost = cp.sum(cp.huber(X.T@beta - Y, 1))\n",
    "    cp.Problem(cp.Minimize(cost)).solve()\n",
    "    huber_data[idx] = fit.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Plot the relative reconstruction error for \n",
    "# least-squares, prescient, and Huber regression.\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.plot(p_vals, lsq_data, label='Least squares')\n",
    "ax.plot(p_vals, huber_data, label='Huber')\n",
    "ax.plot(p_vals, prescient_data, label='Prescient')\n",
    "ax.set_ylabel(r'$\\||\\beta - \\beta^{\\mathrm{true}}\\||_2/\\||\\beta^{\\mathrm{true}}\\||_2$')\n",
    "ax.set_xlabel('p')\n",
    "ax.legend(loc='upper left');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Plot the relative reconstruction error for Huber and prescient regression,\n",
    "# zooming in on smaller values of p.\n",
    "indices = np.where(p_vals <= 0.08)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.plot(p_vals[indices], huber_data[indices], 'g', label='Huber')\n",
    "ax.plot(p_vals[indices], prescient_data[indices], 'r', label='Prescient')\n",
    "ax.set_ylabel(r'$\\||\\beta - \\beta^{\\mathrm{true}}\\||_2/\\||\\beta^{\\mathrm{true}}\\||_2$')\n",
    "ax.set_xlabel('p')\n",
    "ax.set_xlim([0, 0.07])\n",
    "ax.set_ylim([0, 0.05])\n",
    "ax.legend(loc='upper left');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
