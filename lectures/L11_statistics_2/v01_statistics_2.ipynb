{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Statistics (continued...)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.optimize as opt\n",
    "import scipy.stats as st\n",
    "\n",
    "from ipywidgets import interact, FloatSlider, IntSlider\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Last class\n",
    "\n",
    "* Random variable:\n",
    "  - A variable that takes a stochastic value from a set of possible outcomes.\n",
    "  - A function of a random variable is a random variable.\n",
    "* Statistic\n",
    "  - A function of observations of a random variable.\n",
    "  - Statistics are random variables.\n",
    "* Model vs manifold of models\n",
    "  - A model is a probability distribution over outcomes indexed by parameters.\n",
    "  - A manifold of models is a class of models described by their parameter values.\n",
    "* Likelihood function\n",
    "  - The **likelihood function** for a manifold of statistical models is a function that maps values of the parameters and observations into the probability of having observed those observations given the parameters.\n",
    "* Direct and inverse problems\n",
    "  - The _direct problem_ is to draw a sample from a given model.\n",
    "  - The inverse problem takes a given manifold of models and a sample generated from one of these models and infers which model from the manifold generated the data.\n",
    "* Estimator\n",
    "  - An estimator is a function that maps a sample into a model from a given manifold of models.\n",
    "  - An estimator is a statistic, a statistic is a random variable, so estimators are random variables.\n",
    "* Maximum likelihood\n",
    "  - An estimator that is defined by finding the model that maximizes the likelihood function\n",
    "  - Great because of the theoretical properties that it has: consistent, asymptotically normal, etc...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## This class\n",
    "\n",
    "* Sufficient statistics\n",
    "* Moment based estimators\n",
    "* Estimate uncertainty\n",
    "* Hypothesis testing and confidence intervals\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Sufficient Statistics\n",
    "\n",
    "Let $T(\\tilde{y})$ be a (potentially vector-valued) function of i.i.d. observations from a model.\n",
    "\n",
    "A statistic $t = T(\\tilde{y})$ is sufficient for the parameter $\\theta$ if the conditional probability distribution $p(\\tilde{y} | T(\\tilde{y}; \\theta)$ does not depend on the parameter $\\theta$ for any value of $t$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Example**\n",
    "\n",
    "Consider $\\tilde{y}$, $n$ i.i.d. observations drawn from the Bernoulli distribution with parameter $\\theta$.\n",
    "\n",
    "Let $T(\\tilde{y}) \\equiv \\sum_{i=1}^n \\tilde{y}_i$\n",
    "\n",
    "Then\n",
    "\n",
    "\\begin{align*}\n",
    "  p(\\tilde{y}  | T(\\tilde{y})=t) &= \\frac{p(\\tilde{y}, T(\\tilde{y})=t)}{p(T(\\tilde{y})=t)} \\\\\n",
    "  &= \\frac{p(\\tilde{y}) \\mathbb{1}_{T(\\tilde{y})=t}}{p(T(\\tilde{y})=t)} \\\\\n",
    "  &= \\frac{\\mathbb{1}_{T(\\tilde{y})=t} \\theta^t (1 - \\theta)^{n-t}}{{n\\choose{t}} \\theta^t (1 - \\theta)^{n-t}} \\\\\n",
    "  &= \\frac{\\mathbb{1}_{T(\\tilde{y})=t}}{{n\\choose{t}}}\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Factorization theorem\n",
    "\n",
    "It's not always easy to work with the raw definition of sufficient statistic. Luckily, there's a theorem that can help us:\n",
    "\n",
    "**Theorem**: $T(\\tilde{y})$ is sufficient for $\\theta$ if and only if the joint pdf/pmf of $\\tilde{y}$ can be factored as:\n",
    "\n",
    "$$p(\\tilde{y}; \\theta) = h(\\tilde{y}) g(T(\\tilde{y}); \\theta)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Example**: Recall our manifold of models that described quarterly GDP growth. The manifold was described by\n",
    "\n",
    "$$y = \\theta + \\varepsilon$$\n",
    "\n",
    "where $\\varepsilon \\sim N(0, 1)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Define $\\bar{y} \\equiv \\frac{1}{n} \\sum_{i=1}^n \\tilde{y}_i$\n",
    "\n",
    "Given a $\\theta$, the probability density function of $Y$ is given by\n",
    "\n",
    "\\begin{align*}\n",
    "  p(\\tilde{y}) &= \\prod_{i=1}^n \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp \\left( -\\frac{(\\tilde{y}_i - \\theta)^2}{2 \\sigma^2} \\right) \\\\\n",
    "  &= \\left(\\frac{1}{2 \\pi \\sigma^2} \\right)^{n/2} \\exp \\left( -\\frac{ \\sum_{i=1}^n (\\tilde{y}_i - \\theta)^2}{2 \\sigma^2} \\right) \\\\\n",
    "  &= \\left(\\frac{1}{2 \\pi \\sigma^2} \\right)^{n/2} \\exp \\left( -\\frac{ \\sum_{i=1}^n (\\tilde{y}_i - \\bar{y} + \\bar{y} - \\theta)^2}{2 \\sigma^2} \\right) \\\\\n",
    "  &= \\left(\\frac{1}{2 \\pi \\sigma^2} \\right)^{n/2} \\exp \\left( -\\frac{ \\sum_{i=1}^n (\\tilde{y}_i - \\bar{y})^2 + \\sum_{i=1}^n (\\bar{y} - \\theta)^2 + 2 \\sum_{i=1}^n (\\tilde{y_i}\\bar{y} - \\theta \\tilde{y}_i - 2 \\bar{y}^2 + \\bar{y}\\theta)}{2 \\sigma^2} \\right) \\\\\n",
    "  &= \\left(\\frac{1}{2 \\pi \\sigma^2} \\right)^{n/2} \\exp \\left( -\\frac{ \\sum_{i=1}^n (\\tilde{y}_i - \\bar{y})^2 + \\sum_{i=1}^n (\\bar{y} - \\theta)^2 - 2 \\bar{y}\\sum_{i=1}^n (\\tilde{y}_i - \\bar{y}) - 2 \\theta \\sum_{i=1}^n (\\tilde{y}_i - \\bar{y})}{2 \\sigma^2} \\right) \\\\\n",
    "  &= \\left(\\frac{1}{2 \\pi \\sigma^2} \\right)^{n/2} \\exp \\left( -\\frac{ \\sum_{i=1}^n (\\tilde{y}_i - \\bar{y})^2 + \\sum_{i=1}^n (\\bar{y} - \\theta)^2}{2 \\sigma^2} \\right) \\\\\n",
    "  &= \\underbrace{\\left(\\frac{1}{2 \\pi \\sigma^2} \\right)^{n/2} \\exp \\left( -\\frac{ \\sum_{i=1}^n (\\tilde{y}_i - \\bar{y})^2}{2 \\sigma^2} \\right)}_{h(\\tilde{y})} \\underbrace{\\exp \\left( \\frac{-n (\\bar{y} - \\theta)^2}{2 \\sigma^2} \\right)}_{g(T(\\tilde{y}); \\theta)} \\\\\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Sufficient vs minimally sufficient**\n",
    "\n",
    "The statistic $T(\\tilde{y}) = \\tilde{y}$ is always sufficient, but this doesn't mean that it's useful...\n",
    "\n",
    "Minimally sufficient is, loosely defined, as a sufficient statistic with the smallest possible cardinality (not unique!)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "###  Helpful benefits of sufficient statistics (and  factorization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data reduction: computation**\n",
    "\n",
    "Since the maximum likelihood estimator is done by maximizing the likelihood function, if we can factorize the likelihood function into $h(\\tilde{y})$ and $g(T(\\tilde{y}); \\theta)$ then to get the maximum likelihood estimator, we could optimize solely over $g$.\n",
    "\n",
    "Occasionally $g$ will be simpler to optimize over... In the normal example above (with known standard deviation), it's obvious that the maximum likelihood estimate of $\\theta$ will be $\\bar{y}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Data reduction: privacy**\n",
    "\n",
    "If you can identify the sufficient statistics to generate an estimator for a model, then you don't necessarily need to receive the data!\n",
    "\n",
    "Why is this helpful?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Helpful because there is lots of data that is protected by privacy concerns. For example, in the United States it is very difficult to gain access to government collected data on income due to concerns about privacy or data abuse, but it is (at least hypothetically) much easier to receive aggregated data because it would be difficult to reverse engineer the aggregation!\n",
    "\n",
    "The reason it's harder is because many different data sets can generate the same aggregated numbers. If I chose 10 random numbers and added them together, could you identify the 10 numbers if I just told you their mean and the standard deviation of the numbers I chose?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Moment Based Estimators\n",
    "\n",
    "Moment based estimators don't explicitly rely on the likelihood function and can be written down in cases where the likelihood function is intractable.\n",
    "\n",
    "When certain conditions are met, moment based estimators are consistent and asymptotically normally distributed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Let $\\tilde{y}$ be an $n$ element observation drawn from a model indexed by $m$ parameters $\\theta$ from within a manifold of models described by $\\Theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Method of moments\n",
    "\n",
    "Let $Y$ be the random variable generated by the probability distribution $f(Y | \\theta$).\n",
    "\n",
    "Let $\\mu_j \\equiv E[Y^j] = g_j(\\theta)$ be the j-th moment of $Y$ and $\\hat{\\mu_k} \\equiv \\frac{1}{n} \\sum_{i=1}^n \\tilde{y}_i^j$\n",
    "\n",
    "then the method of moments estimators is $\\hat{\\theta} \\in \\Theta$ such that \n",
    "\n",
    "\\begin{align*}\n",
    "  \\hat{\\mu}_1  &= g_1(\\hat{\\theta}) \\\\\n",
    "  \\hat{\\mu}_2 &= g_2(\\hat{\\theta}) \\\\\n",
    "  &\\dots \\\\\n",
    "  \\hat{\\mu}_m &= g_m(\\hat{\\theta})\n",
    "\\end{align*}\n",
    "\n",
    "This is a system of equations with $m$ equations and $m$ unknowns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Example: Uniform distribution**\n",
    "\n",
    "Consider the uniform distribution on the interval $[a, b]$. If $Y \\sim U(a, b)$ then\n",
    "\n",
    "\\begin{align*}\n",
    "  \\mu_1 &= E[Y] = \\frac{1}{2}(a + b) \\\\\n",
    "  \\mu_2 &= E[Y^2] = \\frac{1}{3}(a^2 + ab + b^2) \\\\\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Give $\\tilde{y}$, we can compute the sample moments $\\hat{\\mu}_1$ and $\\hat{\\mu}_2$ and then solve these equations for $a$ and $b$ (the parameters) to get\n",
    "\n",
    "\\begin{align*}\n",
    "  \\hat{a} &= \\hat{\\mu}_1 - \\sqrt{3 (\\hat{\\mu}_2 - \\hat{\\mu}_1^2)} \\\\\n",
    "  \\hat{b} &= \\hat{\\mu}_2 + \\sqrt{3 (\\hat{\\mu}_2 - \\hat{\\mu}_1^2)}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Note**: The method of moments estimators are often relatively easy to compute which makes them a great \"first try\" estimator.\n",
    "\n",
    "One place that this could be useful is if you are maximizing a complex likelihood that is not \"well behaved\" in particular regions. In a case like this, you might consider starting your optimization at the method of moments estimator to give the optimization algorithm a \"hint\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "a, b = 0.0, 2.0\n",
    "n = 50\n",
    "Y = st.uniform(a, b)\n",
    "\n",
    "y = Y.rvs(n)\n",
    "\n",
    "mu1_hat = np.mean(y)\n",
    "mu2_hat = np.mean(y**2)\n",
    "\n",
    "inner = 3*(mu2_hat - mu1_hat**2)\n",
    "ahat = mu1_hat - np.sqrt(inner)\n",
    "bhat = mu1_hat + np.sqrt(inner)\n",
    "\n",
    "print(f\"Parameters from model that generated data are {a} and {b}\")\n",
    "print(f\"Method of moment estimators are {ahat} and {bhat}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def method_of_moments(moment_generator, data, theta_0):\n",
    "    \"\"\"\n",
    "    Implements the method of moments\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    moment_generator : function\n",
    "        Returns a vector of first m moments where\n",
    "        m is the number of parameters\n",
    "    data : np.array\n",
    "        Observations that were generated by a model\n",
    "        from within the manifold of models\n",
    "    theta_0 : np.array(ndim=1, size=m)\n",
    "        An initial guess for parameter values\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    sol.x : np.array(ndim=1, size=k)\n",
    "        The parameters that solve the moment conditions\n",
    "    \"\"\"\n",
    "    moments = np.array([np.mean(data**(k+1)) for k in range(len(theta_0))])\n",
    "    find_root = lambda x: moment_generator(x) - moments\n",
    "    sol = opt.root(find_root, theta_0)\n",
    "    \n",
    "    return sol.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uniform_moments(theta):\n",
    "    mu_1 = 1/2 * (theta[0] + theta[1])\n",
    "    mu_2 = 1/3 * (theta[0]**2 + theta[0]*theta[1] + theta[1]**2)\n",
    "    return np.array([mu_1, mu_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ahat_n, bhat_n = method_of_moments(uniform_moments, y, np.array([0, 1]))\n",
    "\n",
    "print(f\"Parameters from model that generated data are {a} and {b}\")\n",
    "print(f\"Numerical method of moment estimators are {ahat_n:0.2f} and {bhat_n:0.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Generalized method of moments estimator\n",
    "\n",
    "Let $m(\\theta) \\equiv E [ g(\\tilde{y}, \\theta) ] = 0$ be a vector of $k$ \"moment conditions\" then the generalized method of moments (GMM) estimator is given by\n",
    "\n",
    "$$\\hat{\\theta} = \\text{arg}\\min_{\\theta \\in \\Theta} \\left(\\frac{1}{n} \\sum_{i=1}^n g(\\tilde{y}_i, \\theta) \\right)^T \\hat{W} \\left(\\frac{1}{n} \\sum_{i=1}^n g(\\tilde{y}_i, \\theta) \\right)$$\n",
    "\n",
    "where $\\hat{W}$ is some positive semi-definite weighting matrix.\n",
    "\n",
    "- $m$ parameters\n",
    "- $n$ observations\n",
    "- $k$ moments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Which moments?**\n",
    "\n",
    "In the method of moments, we were told to pick a particular set of moments (the first $m$ moments).\n",
    "\n",
    "In the generalized method of moments, the choice of moments is up to us.\n",
    "\n",
    "Picking the \"right\" moments is part art and part science:\n",
    "  * You often want to choose moments related to the features of interest from the data.\n",
    "  * Some individuals recommend to put moments in terms of \"percentage error\" so that each moment equation is in the same units\n",
    "  * Likewise, one could choose to disproportionately weight moments that are of highest interest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Choice of weight matrix**\n",
    "\n",
    "How can we choose $\\hat{W}$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "In some sense, it doesn't matter which $\\hat{W}$ we choose because, as long as $\\hat{W}$ is postitive semi-definite, then $\\hat{\\theta}$ will be consistent.\n",
    "\n",
    "This means that one \"easy\" choice is the identity matrix.\n",
    "\n",
    "However, there are benefits to choosing a \"better\" weighting matrix:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Two-step feasible GMM:\n",
    "  * Step 1: Take $W = I$ and compute a preliminary GMM estimate $\\hat{\\theta}_0$.\n",
    "  * Step 2: Choose $W = \\hat{W}_n(\\hat{\\theta}_0) \\equiv \\left(\\frac{1}{n} \\sum_{i=1}^n g(\\tilde{y}, \\hat{\\theta}) g(\\tilde{y}, \\hat{\\theta})^T \\right)^{-1}$. $\\hat{W}_n$ converges in probability to $\\Omega^{-1}$ and therefore if we compute $\\hat{\\theta}$ using this estimate, then the estimator will be asymptotically efficient.\n",
    "- Iterated GMM:\n",
    "  * Similar to two-step feasible GMM, but iterate repeatedly until a convergence criterion is met.\n",
    "  * Asymptotically, this estimator should be no better than two-step feasible GMM, but, there is some evidence that it performs better with finite samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def gmm_criterion(g, data, W, theta):\n",
    "    # Make sure data is a row vector\n",
    "    data = data[None, :]\n",
    "\n",
    "    # Compute the mean across data\n",
    "    ghat = np.mean(g(data, theta), axis=1)\n",
    "\n",
    "    # Return weighted sum\n",
    "    return ghat.T @ W @ ghat\n",
    "\n",
    "\n",
    "def gmm(g, data, W, theta_0):\n",
    "    \"\"\"\n",
    "    Implements the method of moments\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    g : function\n",
    "        Returns a k-shape vector of moments of interest given\n",
    "        data and parameters as an input\n",
    "    data : np.array\n",
    "        Observations that were generated by a model\n",
    "        from within the manifold of models\n",
    "    theta_0 : np.array(ndim=1, size=m)\n",
    "        An initial guess for parameter values\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    sol.x : np.array(ndim=1, size=k)\n",
    "        The parameters that solve the moment conditions\n",
    "    \"\"\"\n",
    "    # Define function to minimize\n",
    "    min_me = lambda theta: gmm_criterion(g, data, W, theta)\n",
    "\n",
    "    sol = opt.minimize(min_me, theta_0, method=\"nelder-mead\")\n",
    "\n",
    "    return sol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Example: Test scores**\n",
    "\n",
    "This example is based on a [notebook by Rick Evans](https://notes.quantecon.org/submission/5b3b1856b9eab00015b89f90) on the QuantEcon Notes site.\n",
    "\n",
    "The data comes from a year of test scores for an intermediate macroeconomics course. The maximum possible score on the test was 450 and minimum possible score was 0.\n",
    "\n",
    "We will assume that these scores were generated from a truncated normal distribution. Let $\\theta = \\begin{bmatrix} \\mu & \\sigma \\end{bmatrix}$ where\n",
    "\n",
    "* $a = 0$ be the minimum value the trunctated normal random variable can take\n",
    "* $b = 450$ be the maximum value the trunctated normal random variable can take\n",
    "* $\\mu$ be the mean of the non-truncated normal random variable\n",
    "* $\\sigma$ be the standard deviation of the non-truncated normal random variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def create_truncnorm(a, b, mu, std):\n",
    "    \"\"\"\n",
    "    Automates creating a truncated normal random\n",
    "    variable where mu and std are the mean and\n",
    "    standard deviation of the original normal random\n",
    "    variable and a and b are the lower and upper bounds\n",
    "    of the truncated normal\n",
    "    \"\"\"\n",
    "    a_sp, b_sp = (a - mu) / std, (b - mu) / std\n",
    "\n",
    "    return st.truncnorm(a=a_sp, b=b_sp, loc=mu, scale=std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "test_scores = np.loadtxt(\"Econ381totpts.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def plot_scores(data, mu, std):\n",
    "\n",
    "    xvals = np.linspace(-10.0, 500, 5000)\n",
    "    tn = create_truncnorm(0.0, 450.0, mu, std)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "    ax.hist(test_scores, bins=25, density=True)\n",
    "    ax.plot(xvals, tn.pdf(xvals))\n",
    "\n",
    "    ax.set_xlim(-10, 475)\n",
    "    ax.set_ylim(0.0, 0.009)\n",
    "\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "\n",
    "    ax.set_title(\"Empirical distribution of test scores\")\n",
    "    ax.set_xlabel(\"Scores\")\n",
    "    ax.set_ylabel(r\"$f(y | \\theta)$\")\n",
    "\n",
    "    ax.annotate(f\"$\\mu = {mu:0.2f}$\", (50, 0.006))\n",
    "    ax.annotate(f\"$\\sigma = {std:0.2f}$\", (50, 0.005))\n",
    "\n",
    "    return fig, ax\n",
    "\n",
    "interact(\n",
    "    lambda mu, std: plot_scores(test_scores, mu, std),\n",
    "    mu=FloatSlider(value=200.0, min=0.0, max=550.0, step=50.0),\n",
    "    std=FloatSlider(value=50.0, min=50.0, max=150.0, step=10.0)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Choosing a $g$**\n",
    "\n",
    "Let's start by choosing a $g$ function that computes the difference between the theoretical mean/variance and the sample mean/variance.\n",
    "\n",
    "$$g(\\tilde{y}_i, \\theta) = \\begin{bmatrix} (\\tilde{y}_i - \\hat{\\mu}) \\\\ (\\tilde{y}_i - \\hat{\\mu})^2 - \\hat{\\sigma}^2 \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def mv_g(data, theta):\n",
    "    \"\"\"\n",
    "    Computes mean and variance criterion\n",
    "    \"\"\"\n",
    "    # Theoretical mean/std\n",
    "    mu, std = theta\n",
    "    tn = create_truncnorm(0.0, 450.0, mu, std)\n",
    "\n",
    "    # Compute g\n",
    "    return np.vstack([data - mu, (data - mu)**2 - std**2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "gv_sol = gmm(mv_g, test_scores, np.eye(2), np.array([250.0, 50.0]))\n",
    "\n",
    "mu, std = gv_sol.x\n",
    "tn = create_truncnorm(0.0, 450.0, mu, std)\n",
    "\n",
    "print(f\"Solution is mu={gv_sol.x[0]:0.2f} and std={gv_sol.x[1]:0.2f}\")\n",
    "print(f\"Mean and variance of data is {np.mean(test_scores)} and {np.var(test_scores)}\")\n",
    "print(f\"Mean and variance of model is {tn.mean():0.2f} and {tn.var():0.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "plot_scores(test_scores, mu, std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def mv_g_err(data, theta):\n",
    "    \"\"\"\n",
    "    Computes mean and variance criterion\n",
    "    \"\"\"\n",
    "    # Theoretical mean/std\n",
    "    mu, std = theta\n",
    "    tn = create_truncnorm(0.0, 450.0, mu, std)\n",
    "\n",
    "    # Compute g\n",
    "    return np.vstack([(data - mu) / mu, ((data - mu)**2 - std**2) / std**2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "gv_sol_err = gmm(mv_g_err, test_scores, np.eye(2), np.array([500.0, 150.0]))\n",
    "\n",
    "mu, std = gv_sol_err.x\n",
    "tn = create_truncnorm(0.0, 450.0, mu, std)\n",
    "\n",
    "print(f\"Solution is mu={gv_sol_err.x[0]:0.2f} and std={gv_sol_err.x[1]:0.2f}\")\n",
    "print(f\"Mean and variance of data is {np.mean(test_scores)} and {np.var(test_scores)}\")\n",
    "print(f\"Mean and variance of model is {tn.mean():0.2f} and {tn.var():0.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scores(test_scores, mu, std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Choosing a $g$**\n",
    "\n",
    "Now can we do better with different moments? Let's try the fraction of scores in multiple buckets $\\{ [0, 100], (100, 200], (200, 300], (300, 400], (400, 450] \\}$ and label the fraction of scores in each bucket as $b_{\\text{bucket}}$.\n",
    "\n",
    "$$g(\\tilde{y}_i, \\theta) = \\begin{bmatrix} \\tilde{y}_i \\in \\text{bucket} - b_{[0, 100]} \\\\ \\tilde{y}_i \\in \\text{bucket} - b_{(100, 200]} \\\\ \\tilde{y}_i \\in \\text{bucket} - b_{(200, 300]} \\\\ \\tilde{y}_i \\in \\text{bucket} - b_{(300, 400]} \\\\ \\tilde{y}_i \\in \\text{bucket} - b_{(400, 450]} \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def bucket_g(data, theta):\n",
    "    \"\"\"\n",
    "    Computes mean and variance criterion\n",
    "    \"\"\"\n",
    "    # Theoretical mean/std\n",
    "    mu, std = theta\n",
    "    tn = create_truncnorm(0.0, 450.0, mu, std)\n",
    "\n",
    "    # Theoretical moments\n",
    "    lbs = [0, 100, 200, 300, 400]\n",
    "    ubs = [100, 200, 300, 400, 450]\n",
    "    nbuckets = len(lbs)\n",
    "    bucket_moments = np.array(\n",
    "        [tn.cdf(ubs[i]) - tn.cdf(lbs[i]) for i in range(nbuckets)]\n",
    "    )\n",
    "\n",
    "    # Compute g\n",
    "    return np.vstack(\n",
    "        [\n",
    "            ((data<ubs[i]) & (data>=lbs[i])).astype(int) - bucket_moments[i]\n",
    "            for i in range(nbuckets)\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "bucket_sol = gmm(bucket_g, test_scores, np.eye(5), np.array([500.0, 150.0]))\n",
    "\n",
    "mu, std = bucket_sol.x\n",
    "tn = create_truncnorm(0.0, 450.0, mu, std)\n",
    "\n",
    "print(f\"Solution is mu={bucket_sol.x[0]:0.2f} and std={bucket_sol.x[1]:0.2f}\")\n",
    "print(f\"Mean and variance of data is {np.mean(test_scores)} and {np.var(test_scores)}\")\n",
    "print(f\"Mean and variance of model is {tn.mean():0.2f} and {tn.var():0.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scores(test_scores, mu, std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Simulated method of moments\n",
    "\n",
    "Both the method of moments and the generalized method of moments require that one is able to directly evaluate theoretical moment functions... When this is not possible, one can use the simulated method of moments.\n",
    "\n",
    "In generalized method of moments, computed\n",
    "\n",
    "$$g(\\theta_0) \\equiv E[g(\\tilde{y}, \\theta_0)] = 0$$\n",
    "\n",
    "using theoretical moments (i.e. $\\tilde{y}_i - \\mu(\\theta)$).\n",
    "\n",
    "As the name suggests, simulated method of moments replaces the theoretical computation with data simulated from a model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Algorithm**\n",
    "\n",
    "1. Choose moments to compute and compute the corresponding moments in the \"observed data\"\n",
    "2. Optimize with respect to $\\hat{\\theta}$ by\n",
    "  - Propose $\\hat{\\theta}$\n",
    "  - Simulate observations from model $f(\\tilde{y} | \\hat{\\theta}))$\n",
    "  - Compute moments on the simulated data in the same way that you did for the observed data\n",
    "  - If sufficiently close, finish, otherwise find new proposal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def smm_criterion(g, data, W, theta):\n",
    "    # Create model for given theta\n",
    "    mu, std = theta\n",
    "    tn = create_truncnorm(0.0, 450.0, mu, std)\n",
    "\n",
    "    # Draw new samples\n",
    "    np.random.seed(61089)\n",
    "    sim = tn.rvs(10_000)\n",
    "\n",
    "    # Compute difference in the data and simulated\n",
    "    # moments\n",
    "    ghat = (g(data) - g(sim))\n",
    "\n",
    "    # Return weighted sum\n",
    "    return ghat.T @ W @ ghat\n",
    "\n",
    "\n",
    "def smm(g, data, W, theta_0):\n",
    "    \"\"\"\n",
    "    Implements the method of moments\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    g : function\n",
    "        Returns a k-shape vector of moments of interest given\n",
    "        data as an input\n",
    "    data : np.array\n",
    "        Observations that were generated by a model\n",
    "        from within the manifold of models\n",
    "    theta_0 : np.array(ndim=1, size=m)\n",
    "        An initial guess for parameter values\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    sol.x : np.array(ndim=1, size=k)\n",
    "        The parameters that solve the moment conditions\n",
    "    \"\"\"\n",
    "    # Define function to minimize\n",
    "    min_me = lambda theta: smm_criterion(g, data, W, theta)\n",
    "\n",
    "    sol = opt.minimize(min_me, theta_0, method=\"nelder-mead\")\n",
    "\n",
    "    return sol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def g_moments(data):\n",
    "    return np.array([np.mean(data), np.var(data)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "sol_smm = smm(g_moments, test_scores, np.eye(2), np.array([200.0, 100.0]))\n",
    "\n",
    "mu, std = sol_smm.x\n",
    "tn = create_truncnorm(0.0, 450.0, mu, std)\n",
    "\n",
    "print(f\"Solution is mu={sol_smm.x[0]:0.2f} and std={sol_smm.x[1]:0.2f}\")\n",
    "print(f\"Mean and variance of data is {np.mean(test_scores)} and {np.var(test_scores)}\")\n",
    "print(f\"Mean and variance of model is {tn.mean():0.2f} and {tn.var():0.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "plot_scores(test_scores, mu, std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Estimate Uncertainty\n",
    "\n",
    "We've emphasized that parameter estimates, $\\hat{\\theta}$, are random variables, but we've really only talked about the \"point estimates\" so far.\n",
    "\n",
    "What probability distribution is associated with $\\hat{\\theta}$?\n",
    "\n",
    "Multiple ways to proceed:\n",
    "\n",
    "* Asymptotic normality\n",
    "* Bootstrapping\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Asymptotic normality (of the maximum likelihood estimate)\n",
    "\n",
    "Asymptotic comes from the central limit theorem (CLT) which states,\n",
    "\n",
    "> Suppose $\\tilde{y}$ is a sequence of $n$ of independent and identically distributed random variables drawn from $Y$. Let $E[y] = \\mu < \\infty$ and $V[y] = \\sigma^2 < \\infty$ then as $n \\rightarrow \\infty$, the random variable $\\frac{\\sqrt{n}}{\\sigma}(Y - \\mu)$ converges in distribution to $N(0, 1)$\n",
    "\n",
    "Since $\\hat{\\theta}$ is a random variable with finite mean and variance (given certain regularity conditions) then as $n \\rightarrow \\infty$, $\\frac{\\sqrt{n} (\\hat{\\theta} - \\theta)}{\\sigma} \\sim N(0, 1)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Score function and Fisher information**\n",
    "\n",
    "Let $f(Y | \\theta)$ be the probability density function for the random variable $Y$, then the _score_ is the gradient of the log-likelihood function.\n",
    "\n",
    "$$s(\\theta) = \\frac{\\partial \\log \\mathcal{L}(\\theta; \\tilde{y})}{\\partial \\theta}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The _Fisher information_ is defined to be\n",
    "\n",
    "$$I_n(\\theta) = V_{\\theta} \\left( \\sum_{i=1}^n s(\\tilde{y}_i) \\right) = \\sum_{i=1}^n V_{\\theta} (s(\\tilde{y}_i; \\theta))$$\n",
    "\n",
    "Furthermore,\n",
    "\n",
    "$$I_n(\\theta) = n I(\\theta)$$\n",
    "\n",
    "and\n",
    "\n",
    "$$I(\\theta) = -E_{\\theta} \\left( \\frac{\\partial^2 \\log f(Y | \\theta)}{\\partial \\theta^2} \\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Asymptotic Normality of the MLE**:\n",
    "\n",
    "> Let $\\text{se} = \\sqrt{V(\\hat{\\theta}_n)}$. Under appropriate regularity conditions, the following hold:\n",
    ">\n",
    ">1. $\\text{se} \\approx \\sqrt{1 / I_n(\\theta)}$ and $\\frac{\\hat{\\theta}_n - \\theta}{\\text{se}} \\rightarrow N(0, 1)$\n",
    ">2. Let $\\hat{\\text{se}} = \\sqrt{1 / I_n(\\hat{\\theta})}$ then $$\\frac{(\\hat{\\theta}_n - \\theta)}{\\hat{\\text{se}}} \\rightarrow N(0, 1)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Example**:\n",
    "\n",
    "Let $\\tilde{y}$ be $n$ elements generated from a Bernoulli random variable. The MLE is given by\n",
    "\n",
    "$$\\hat{p}_n = \\sum_i \\frac{\\tilde{y}_i}{n}$$\n",
    "\n",
    "and $$f(\\tilde{y}_i; p) = p^{\\tilde{y}_i} (1 - p)^{1 - \\tilde{y}_i}$$\n",
    "\n",
    "then $$log(\\tilde{y} | p) = \\tilde{y}_i \\log(p) + (1 - \\tilde{y}_i) \\log(1 - p)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\\begin{align*}\n",
    "  s(p; \\tilde{y}_i) &= \\frac{\\tilde{y}_i}{p} + \\frac{1 - \\tilde{y}_i}{1 - p} \\\\\n",
    "  -s'(p; \\tilde{y}_i) &= \\frac{\\tilde{y}_i}{p^2} + \\frac{1 - \\tilde{y}_i}{(1 - p)^2} \\\\\n",
    "\\end{align*}\n",
    "\n",
    "then note\n",
    "\n",
    "\\begin{align*}\n",
    "  I(p) &= E[-s'(p; \\tilde{y}_i)] \\\\\n",
    "  &= \\sum -s'(p; y) f(y; p) \\\\\n",
    "  &= \\frac{p}{p^2} + \\frac{1 - p}{(1 - p)^2} \\\\\n",
    "  &= \\frac{1}{p (1-p)}\n",
    "\\end{align*}\n",
    "\n",
    "and"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\\begin{align*}\n",
    "  \\hat{\\text{se}} &= \\sqrt{1 / I_n(\\hat{p})} \\\\\n",
    "  &= \\sqrt{1 / (n I(\\hat{p})} \\\\\n",
    "  &= \\left( \\frac{\\hat{p} (1 - \\hat{p})}{n} \\right)^{\\frac{1}{2}}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Bootstrapping\n",
    "\n",
    "Bootstrapping leverages the fact that, given $\\hat{\\theta}$, one can sample data\n",
    "\n",
    "Given a point-estimate, $\\hat{\\theta}$ the bootstrap estimator of $\\hat{\\text{se}}$ is generated by:\n",
    "\n",
    "1. Simulate $n$ observations of data $\\tilde{y}_j$\n",
    "2. Compute $\\hat{\\theta}(\\tilde{y}_j)$\n",
    "3. Compute $\\hat{\\text{se}} = \\frac{1}{n} \\sum_{j=1} (\\hat{\\theta}(\\tilde{y}_j) - \\hat{\\theta})^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Hypothesis Testing and Confidence Intervals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Hypothesis Testing\n",
    "\n",
    "A frequentist hypothesis test conditions on an assumed value of $\\theta_0 \\in \\Theta$, the null hypothesis.\n",
    "\n",
    "It then makes statements about the probability distribution of a statistic, $g(Y)$, induced by a statistical model $f(Y | \\theta_0)$.\n",
    "\n",
    "\"How unusual is this realization of the statistic, $g(Y)$, given the null hypothesis?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Choose a test size ($\\alpha$) then the support of the random variable, $\\mathcal{Y}$, is divided into two disjoint sets $R$ and $R^C$:\n",
    "\n",
    "* Reject $H_0$ if $Y \\in R$\n",
    "* Retain (do not reject) $H_0$ if $Y \\notin R$\n",
    "\n",
    "Typically the set $R$ is defined by $R \\equiv \\{ y: T(y) > c(\\alpha) \\}$ (a cut-off in statistic space), but this partition depends on the type of test that we're running."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Two types of errors**\n",
    "\n",
    "* Type 1: The null hypothesis is incorrectly rejected in favor of the alternative hypothesis\n",
    "* Type 2: The null hypothesis is incorrectly not rejected over the alternative hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Wald test**\n",
    "\n",
    "Used to test hypothesis of the following form:\n",
    "\n",
    "* $H_0$: $\\theta = \\theta_0$\n",
    "* $H_1$: $\\theta \\neq \\theta_0$\n",
    "\n",
    "Define $W = \\frac{\\hat{\\theta} - \\theta_0}{\\text{se}(\\hat{\\theta})}$ then one should reject the null when $|W| > z_{\\alpha / 2}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**\n",
    "\n",
    "Recall our manifold of models for quarterly GDP growth,\n",
    "\n",
    "$$y = \\theta + \\varepsilon$$\n",
    "\n",
    "where $\\varepsilon \\sim N(0, 1)$.\n",
    "\n",
    "Suppose the null hypothesis is that the economy is stationary, i.e. $H_0$: $\\theta_0 = 0$.\n",
    "\n",
    "We observe the following data: $\\tilde{y} = \\begin{bmatrix} 1.0 & 0.25 & -0.25 & 0.5 & 1.5 & 2.5 & -1.0 \\end{bmatrix}$\n",
    "\n",
    "Then the MLE $\\hat{\\theta} = \\frac{1}{n} \\sum_i \\tilde{y}_i$ and $\\hat{\\text{se}} = \\sqrt{\\hat{\\sigma}^2 / n}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.05\n",
    "ytilde = np.array([1.0, 0.25, -0.25, 0.5, 1.5, 2.5, -1.0])\n",
    "\n",
    "theta_hat = np.mean(ytilde)\n",
    "se_hat = np.sqrt(np.var(ytilde) / ytilde.size)\n",
    "\n",
    "w = (theta_hat - 0.0) / se_hat\n",
    "\n",
    "print(f\"The cut-off point is {st.norm(0, 1).ppf(1 - alpha/2):0.2f}\")\n",
    "print(f\"The w-value is {w:0.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Confidence Intervals\n",
    "\n",
    "A frequentist _confidence interval_ is an interval, $C_n(Y) = (a(Y), b(Y))$, such that\n",
    "\n",
    "$$\\text{Prob}(g(Y) \\in C_n(Y) | \\theta=\\theta_o) \\geq (1 - \\alpha)$$\n",
    "\n",
    "To represent this probability, we compute\n",
    "\n",
    "$$E_{\\mathbb{1}_{[a, b]}}(g(Y)) = \\int \\mathbb{1}_{g(\\tilde{y}) \\in [a, b]} f(Y |  \\theta=\\theta_o) dY$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Example**:\n",
    "\n",
    "Consider the Bernoulli example. Let $\\tilde{y}$ be $n$ independent and identically distributed observations.\n",
    "\n",
    "The maximum likelihood estimator is defined by\n",
    "\n",
    "$$\\hat{p}_n = \\frac{1}{n} \\sum_{i=1}^n \\tilde{y}_i$$\n",
    "\n",
    "and, as previously showed, the standard deviation of $\\hat{p}_n$ is given by\n",
    "\n",
    "$$\\hat{\\text{se}} = \\left( \\frac{\\hat{p} (1 - \\hat{p})}{n} \\right)^{\\frac{1}{2}}$$\n",
    "\n",
    "We know that as $n \\rightarrow \\infty$ that $\\hat{p} \\sim N(p, \\hat{\\text{se}}^2)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "A $(1 - \\alpha)$ (normal approximation) confidence interval is then given by\n",
    "\n",
    "$$C_\\alpha(\\tilde{y}) = \\hat{p}_n \\pm z_{\\alpha/2} \\left( \\frac{\\hat{p} (1 - \\hat{p})}{n} \\right)^{\\frac{1}{2}}$$\n",
    "\n",
    "where $z_{\\alpha/2} \\equiv \\Phi^{-1}(\\alpha/2)$... For $\\Phi^{-1}(0.025) \\approx 1.96$..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "st.norm(0, 1).ppf(1 - 0.05/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\hat{p} \\pm 1.96 \\hat{\\text{se}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "* [Online notes for statistics 705](http://www.stat.cmu.edu/~siva/705/lec11.pdf). Siva Balakrishnan. 2019. Carnegie Mellon University.\n",
    "* [Generalized Method of Moments (GMM) Estimation](https://notes.quantecon.org/submission/5b3b1856b9eab00015b89f90). Richard Evans. 2018. QuantEcon Notes.\n",
    "* [Generalized Method of Moments Estimation](http://home.uchicago.edu/~lhansen/palgrave.pdf). Lars Peter Hansen. 2007. University of Chicago.\n",
    "* All of Statistics by Larry Wasserman\n",
    "* Wikipedia\n",
    "  - [Sufficient statistics](https://en.wikipedia.org/wiki/Sufficient_statistic)\n",
    "  - [Score](https://en.wikipedia.org/wiki/Score_(statistics)) and [Fisher information](https://en.wikipedia.org/wiki/Fisher_information)\n",
    "  - [Method of moments](https://en.wikipedia.org/wiki/Method_of_moments_(statistics)), [Generalized method of moments](https://en.wikipedia.org/wiki/Generalized_method_of_moments)\n",
    "  - [Hypothesis testing](https://en.wikipedia.org/wiki/Statistical_hypothesis_testing), [Wald test](https://en.wikipedia.org/wiki/Wald_test), [Likelihood ratio test](https://en.wikipedia.org/wiki/Likelihood-ratio_test)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
