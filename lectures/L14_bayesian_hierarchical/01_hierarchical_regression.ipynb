{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Bayesian Hierarchical Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import arviz as az\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pymc3 as pm\n",
    "\n",
    "from pandas_datareader import DataReader\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Hierarchical Models: Cafes\n",
    "\n",
    "The following example is based on material from Richard McElreath and the \"Statistical Rethinking\" book.\n",
    "\n",
    "Suppose you were tasked with providing the mean and standard deviation for the waiting time to get a cup of coffee for every cafe in the world.\n",
    "\n",
    "How would one go about doing this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Method 1: Pooled\n",
    "\n",
    "One could treat each cafe the same and use visits to many different cafes to estimate a world-wide mean and standard deviation of waiting times for cafes.\n",
    "\n",
    "This method disregards the fact that cafes have any differences -- Even if all cafes in the world were run by Starbucks, there would be differences in waiting times due to factors like foot traffic and staff."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Method 2: Amnesia\n",
    "\n",
    "If one was able to visit every cafe in the world repeatedly, one could estimate a cafe-specific mean and standard deviation.\n",
    "\n",
    "We call this method the \"amnesia method\" because it is like you are forgetting that you've ever visited any other cafe in the world... This seems suboptimal because repeated visits to every cafe in the world would become infeasible and, although cafes differ, they also have similarities (which this method ignores)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Method 3: Multilevel (Hierarchical)\n",
    "\n",
    "Rather than treat cafes as completely the same (pooled) or completely separate (amnesia), multilevel models recognize that there are similarities and differences and these models attempt to separate these similarities and differences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## CAPM\n",
    "\n",
    "We aren't going to discuss in detail about the interpretation (or motivation) of the CAPM model today (stay tuned for more on this...), but we think it is a compelling example for hierarchical models.\n",
    "\n",
    "The CAPM is described by the following equation:\n",
    "\n",
    "$$r_{i, t} = r_{f, t} + \\beta_i (r_{m, t} - r_{f, t})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "There are a few components to this equation:\n",
    "\n",
    "* $r_{f, t}$: The risk-free return <-- We will use the 3 month treasury bills to approximate this\n",
    "* $r_{m, t}$: The market return <-- We will use the S&P 500 to approximate the market return\n",
    "* $r_{i, t}$: The return of a particular asset\n",
    "* $\\beta_i$: The beta of the asset <-- This is a measure of relative co-movement with the market returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data Ingestion\n",
    "\n",
    "We will download data from three sources:\n",
    "\n",
    "1. [Wikipedia's list of S&P 500 companies](https://en.wikipedia.org/wiki/List_of_S%26P_500_companies)\n",
    "2. [Yahoo Finance](https://finance.yahoo.com/)\n",
    "3. [Federal Reserve Bank of St Louis](https://fred.stlouisfed.org/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Yahoo Finance previously had an API that one could use to download data, but they deprecated this API in 2017...\n",
    "\n",
    "In response to this, open source contributors have created a new Python package called `yfinance` which you can read about [here](https://github.com/ranaroussi/yfinance). We will use this package to download daily stock prices -- If you'd like to replicate this data gathering, you'll need to run `pip install yfinance` to install the `yfinance` package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### S&P 500 Metadata\n",
    "\n",
    "There are several pieces of metadata that we are particularly interested in here:\n",
    "\n",
    "* `ticker`: This will tell us the ticker that we can use to download the data\n",
    "* `gics`: The [GICS sector](https://en.wikipedia.org/wiki/Global_Industry_Classification_Standard) tells us broad information about the industry that a company works in. An example of a few of these industries would be: _energy_, _materials_, _consumer discretionary_, _financials_, etc...\n",
    "* `gics_subindustry`: The GICS subcategory tells us more specific information about the industry that a company works in. For example, _asset management and custody_, _oil and gas drilling_, _textiles_, _automobile manufacturers_, etc...\n",
    "* `hq_location`: The location of the company headquarters\n",
    "* `start_date`: When the company entered the S&P 500\n",
    "* `founded`: The year the company was founded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Read in tables from wikipedia page\n",
    "sp500_tables = pd.read_html(\n",
    "    \"https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\"\n",
    ")\n",
    "\n",
    "# Get table of companies and rename\n",
    "sp500 = sp500_tables[0].rename(columns={\n",
    "    \"Symbol\": \"ticker\",\n",
    "    \"GICS Sector\": \"gics\",\n",
    "    \"GICS Sub-Industry\": \"gics_subindustry\",\n",
    "    \"Headquarters Location\": \"hq_location\",\n",
    "    \"Date first added\": \"start_date\",\n",
    "})\n",
    "sp500.columns = [c.lower() for c in sp500.columns]\n",
    "\n",
    "# Yahoo uses - rather than .\n",
    "sp500[\"ticker\"] = sp500[\"ticker\"].str.replace(\".\", \"-\")\n",
    "\n",
    "# Add Zoom for fun\n",
    "sp500 = sp500.append(\n",
    "    {\n",
    "        \"ticker\": \"ZM\",\n",
    "        \"security\": \"Zoom\",\n",
    "        \"sec filings\": \"reports\",\n",
    "        \"gics\": \"Communication Services\",\n",
    "        \"gics_subindustry\": \"Telecom Services\",\n",
    "        \"hq_location\": \"San Jose, CA\",\n",
    "        \"start_date\": \"2019-03-22\",\n",
    "        \"cik\": 1585521,\n",
    "        \"founded\": \"2011\",\n",
    "    }, ignore_index=True\n",
    ")\n",
    "\n",
    "# Extract tickers\n",
    "sp500_tickers = sp500[\"ticker\"].to_list()\n",
    "\n",
    "sp500.to_parquet(\"../../Data/sp500/sp500_companies.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "sp500.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### S&P Prices\n",
    "\n",
    "The other piece of data is the actual stock prices. We are given a few options for which type of price data to report:\n",
    "\n",
    "* `Close`: The last price that the stock traded at on a particular day\n",
    "* `Open`: The first price that the stock traded at on a particular day\n",
    "* `High`: The highest price that a stock traded at on a particular day\n",
    "* `Low`: The lowest price that a stock traded at on a particular day\n",
    "* `Adj Close`: The close price adjusted for dividends, splits, and other corporate actions <-- We will be using adjusted close prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import yfinance as yf  # Make sure to install if not installed!\n",
    "\n",
    "raw_prices = yf.download(\n",
    "    tickers=sp500_tickers + [\"^GSPC\"],\n",
    "    start=\"2015-12-31\", end=\"2021-01-01\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "prices = raw_prices.loc[:, \"Close\"].reset_index().rename(\n",
    "    columns={\n",
    "        \"Date\": \"dt\"\n",
    "    }\n",
    ").melt(\n",
    "    id_vars=\"dt\",\n",
    "    var_name=\"ticker\",\n",
    "    value_name=\"price\"\n",
    ").replace(\n",
    "    {\"ticker\": {\"^GSPC\": \"sp500\"}}\n",
    ")\n",
    "\n",
    "prices.to_parquet(\"../../Data/sp500/prices.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "prices.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Treasury Bills\n",
    "\n",
    "We will use the (annualized) return on US treasury bills as a baseline \"risk-free\" return.\n",
    "\n",
    "A treasury bill is a form of government debt in which you pay the government a price $p_{\\tau}$ at time $t$ and are repaid \\\\$1,000 at time $t + \\tau$ (with time measured in months).\n",
    "\n",
    "The implied return is then $r = \\times \\frac{1000 - p}{p}$ which can then be annualized based on the maturity $\\tau$ by \n",
    "\n",
    "$$r_{\\text{annualized}} = 1 - \\left(1 + \\frac{1000 -  p}{p} \\right)^{\\frac{12}{\\tau}}$$\n",
    "\n",
    "We will report the rates in annual percent return (i.e. $r_\\text{annualized} = 0.01$ will be reported as 1\\%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "tbills = DataReader(\n",
    "    \"DTB3\", \"fred\", start=2014, end=2021\n",
    ").reset_index().rename(\n",
    "    columns={\"DTB3\": \"riskfree\", \"DATE\": \"dt\"}\n",
    ").groupby(\n",
    "    pd.Grouper(key=\"dt\", freq=\"M\")\n",
    ").mean()\n",
    "\n",
    "# Convert to monthly\n",
    "tbills[\"riskfree\"] = tbills.eval(\n",
    "    \"100 * ((1 + riskfree/100)**(1/12) - 1)\"\n",
    ")\n",
    "\n",
    "tbills.reset_index().to_parquet(\"../../Data/sp500/tbills.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "tbills.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Merging metadata and prices\n",
    "\n",
    "Eventually we will want to \"group\" certain stocks together based on characteristics from their metadata.\n",
    "\n",
    "We allow for this by merging the metadata and price data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "df = prices.merge(\n",
    "    sp500.loc[:, [\"ticker\", \"gics\", \"gics_subindustry\"]],\n",
    "    on=\"ticker\", how=\"left\"\n",
    ").sort_values([\"ticker\", \"dt\"])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Computing returns and merging T-bills\n",
    "\n",
    "We will need to compute returns at various points in time.\n",
    "\n",
    "We define a function to help us do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def compute_returns(df_in, freq=\"M\"):\n",
    "    gb = df_in.groupby(\n",
    "        [\"ticker\", pd.Grouper(key=\"dt\", freq=freq)]\n",
    "    )\n",
    "\n",
    "    # Get start and end prices\n",
    "    sp500_ms = gb[\"price\"].first()\n",
    "    sp500_me = gb[\"price\"].last()\n",
    "\n",
    "    # Return last value of every other column\n",
    "    out = pd.DataFrame(\n",
    "        100 * (sp500_me - sp500_ms) / sp500_ms\n",
    "    ).rename(\n",
    "        columns={\"price\": \"returns\"}\n",
    "    ).reset_index()\n",
    "\n",
    "    cols_to_iterate = [\n",
    "        c for c in df_in.columns\n",
    "        if c not in [\"dt\", \"ticker\", \"price\"]\n",
    "    ]\n",
    "    for c in cols_to_iterate:\n",
    "        out.loc[:, c] = gb[c].last().values\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "returns = compute_returns(df)\n",
    "\n",
    "component_returns = returns.query(\"ticker != 'sp500'\")\n",
    "index_returns = returns.query(\n",
    "    \"ticker == 'sp500'\"\n",
    ").loc[:, [\"dt\", \"returns\"]].rename(\n",
    "    columns={\"returns\": \"market\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "component_returns.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "returns = component_returns.merge(\n",
    "    tbills, on=\"dt\", how=\"left\"\n",
    ").merge(\n",
    "    index_returns, on=\"dt\", how=\"left\"\n",
    ").dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "returns.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## A Common Approach to CAPM:\n",
    "\n",
    "A common approach to CAPM is to specify a completely separate regression for each stock.\n",
    "\n",
    "We will build a Bayesian version of this regression (to keep the baseline methodology similar), but this regression could also be done (more quickly) using least squares...\n",
    "\n",
    "Our model will be described by\n",
    "\n",
    "\\begin{align*}\n",
    "   r_{i, t} - r_{f, t} &= \\beta_i (r_{m, t} - r_{f, t}) + \\sigma_i \\varepsilon_{i, t}\\\\\n",
    "   \\sigma_i &\\sim \\text{HalfStudentT}(4) \\\\\n",
    "   \\beta_i &\\sim N(0, 5)\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Single stock\n",
    "\n",
    "We begin by just computing the $\\beta_{\\text{ZM}}$ and $\\sigma_{\\text{ZM}}$ for Zoom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "m_zoom = pm.Model()\n",
    "\n",
    "# Basic data\n",
    "zm_returns = returns.query(\"ticker == 'ZM'\")\n",
    "ri_m_rf = zm_returns.eval(\"returns - riskfree\").to_numpy()\n",
    "rm_m_rf = zm_returns.eval(\"market - riskfree\").to_numpy()\n",
    "\n",
    "with m_zoom:\n",
    "    # Data\n",
    "    _ri_m_rf = pm.Data(\"ri_m_rf\", ri_m_rf)\n",
    "    _rm_m_rf = pm.Data(\"rm_m_rf\", rm_m_rf)\n",
    "\n",
    "    # Prior\n",
    "    beta_i = pm.Normal(\"beta_i\", 0.0, 5.0)\n",
    "    sigma_i = pm.HalfStudentT(\"sigma_i\", 2)\n",
    "\n",
    "    # Likelihood\n",
    "    ll = pm.Normal(\"ll\", beta_i*_rm_m_rf, sigma_i, observed=_ri_m_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "with m_zoom:\n",
    "    traces_zoom = pm.sample(3500, tune=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "with m_zoom:\n",
    "    az.plot_trace(traces_zoom)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Many stocks\n",
    "\n",
    "`pymc3` also allows us to estimate many regressions at once (there is no interdependence in this regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "returns.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "m_baseline = pm.Model()\n",
    "\n",
    "# Basic data\n",
    "tickers = returns[\"ticker\"].unique()\n",
    "ntickers = tickers.shape[0]\n",
    "ticker_2_int = dict(zip(tickers, range(ntickers)))\n",
    "int_2_ticker = {v: k for k, v in ticker_2_int.items()}\n",
    "ri_m_rf = returns.eval(\"returns - riskfree\").to_numpy()\n",
    "rm_m_rf = returns.eval(\"market - riskfree\").to_numpy()\n",
    "ticker_idx = returns[\"ticker\"].map(lambda x: ticker_2_int[x]).to_numpy()\n",
    "\n",
    "with m_baseline:\n",
    "    # Data\n",
    "    _ri_m_rf = pm.Data(\"ri_m_rf\", ri_m_rf)\n",
    "    _rm_m_rf = pm.Data(\"rm_m_rf\", rm_m_rf)\n",
    "    _ticker_idx = pm.intX(pm.Data(\"ticker_idx\", ticker_idx))\n",
    "\n",
    "    # Prior\n",
    "    beta_i = pm.Normal(\"beta_i\", 0.0, 5.0, shape=ntickers)\n",
    "    sigma_i = pm.HalfStudentT(\"sigma_i\", 5.0, shape=ntickers)\n",
    "\n",
    "    # Likelihood\n",
    "    ll = pm.Normal(\"ll\", beta_i[_ticker_idx]*_rm_m_rf, sigma_i[_ticker_idx], observed=_ri_m_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "with m_baseline:\n",
    "    traces_baseline = pm.sample(2500, tune=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Posterior of \"amnesia\" $\\beta$s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "betas_nopool = pd.DataFrame(\n",
    "    {\n",
    "        \"beta_mean\": traces_baseline[\"beta_i\"].mean(axis=0),\n",
    "        \"beta_std\": traces_baseline[\"beta_i\"].std(axis=0),\n",
    "        \"sigma_mean\": traces_baseline[\"sigma_i\"].mean(axis=0),\n",
    "        \"sigma_std\": traces_baseline[\"sigma_i\"].std(axis=0)\n",
    "    }, index=tickers\n",
    ")\n",
    "\n",
    "betas_nopool.T.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Plausbility?**\n",
    "\n",
    "Recall that in our single stock estimation, we found $\\beta_{\\text{Zoom}} < 0$...\n",
    "\n",
    "Do many other stocks have negative $\\beta$? How plausible is this given the rest of our estimates? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "betas_nopool.query(\"beta_mean < 0.0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### A Hierarchical Approach to CAPM\n",
    "\n",
    "We now turn to building a hierarchical version of this model.\n",
    "\n",
    "The purpose of the hierarchical model is to allow groups of observations to learn from one another -- We have, at most, 60 observations for each of our stocks because we are computing the 5 year beta (similar to what Yahoo Finance reports).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Grouping observations\n",
    "\n",
    "As we've mentioned previously, model-building is an art that takes practice (and watching others do it)...\n",
    "\n",
    "Hierarchical models are no exception and choosing how to group your observations take practice (and experimentation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Hyperparameters and hyperpriors**\n",
    "\n",
    "A _hyperparameter_ is a parameter that's an input to a prior. For example, in our previous example we specified $\\beta_i \\sim N(0, 5)$ so 0 and 5 were hyperparameters.\n",
    "\n",
    "A _hyperprior_ is a prior on a hyperparameter\n",
    "\n",
    "(Any guess what a hyperhyperprior is? It is a prior on the hyperhyperparameters which are the parameters governing the hyperprior...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Hierarchical model**\n",
    "\n",
    "Hyperpriors will be a central feature of hierarchical models and they will be used to group observations. In our example, we originally wrote the following model\n",
    "\n",
    "\\begin{align*}\n",
    "   r_{i, t} - r_{f, t} &= \\beta_i (r_{m, t} - r_{f, t}) + \\sigma_i \\varepsilon_{i, t}\\\\\n",
    "   \\beta_i &\\sim N(0, 5) \\\\\n",
    "   \\sigma_i &\\sim \\text{HalfStudentT}(4)\n",
    "\\end{align*}\n",
    "\n",
    "A hierarchical version of the model might be specified as\n",
    "\n",
    "\\begin{align*}\n",
    "   r_{i, t} - r_{f, t} &= \\beta_i (r_{m, t} - r_{f, t}) + \\sigma_i \\varepsilon_{i, t}\\\\\n",
    "   \\sigma_i &\\sim \\text{HalfStudentT}(2) \\\\\n",
    "   \\beta_i &\\sim N(\\hat{\\mu}_j, \\hat{\\sigma}_j) \\\\\n",
    "   \\hat{\\mu}_j &\\sim N(0, 5) \\\\\n",
    "   \\hat{\\sigma}_j &\\sim \\text{HalfStudentT}(4)\n",
    "\\end{align*}\n",
    "\n",
    "where $j$ could indicate the GICS sector (`gics`) that $i$ is identified by."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Writing down the model in pymc3\n",
    "\n",
    "\\begin{align*}\n",
    "   r_{i, t} - r_{f, t} &= \\beta_i (r_{m, t} - r_{f, t}) + \\sigma_i \\varepsilon_{i, t}\\\\\n",
    "   \\sigma_i &\\sim \\text{HalfStudentT}(2) \\\\\n",
    "   \\beta_i &\\sim N(\\hat{\\mu}_j, \\hat{\\sigma}_j) \\\\\n",
    "   \\hat{\\mu}_j &\\sim N(0, 5) \\\\\n",
    "   \\hat{\\sigma}_j &\\sim \\text{HalfStudentT}(2)\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Ticker/subindustry\n",
    "_tick_sect = returns.loc[:, [\"ticker\", \"gics\"]]\n",
    "tick_sect = _tick_sect.loc[~_tick_sect.duplicated(keep=\"first\"), :]\n",
    "tickers = tick_sect[\"ticker\"].to_numpy()\n",
    "ntickers = tickers.shape[0]\n",
    "sects = tick_sect[\"gics\"].unique()\n",
    "nsect = sects.shape[0]\n",
    "\n",
    "# Mappings\n",
    "ticker_2_int = dict(zip(tickers, range(ntickers)))\n",
    "int_2_ticker = {v: k for k, v in ticker_2_int.items()}  # Only reverse when unique\n",
    "sect_2_int = dict(zip(sects, range(nsect)))\n",
    "int_2_sect = {v: k for k, v in sect_2_int.items()}  # Only reverse when unique\n",
    "ticker_2_sect = dict(\n",
    "    zip(\n",
    "        tick_sect[\"ticker\"].map(ticker_2_int).to_numpy(),\n",
    "        tick_sect[\"gics\"].map(sect_2_int).to_numpy()\n",
    "    )\n",
    ")\n",
    "\n",
    "# Data\n",
    "ri_m_rf = returns.eval(\"returns - riskfree\").to_numpy()\n",
    "rm_m_rf = returns.eval(\"market - riskfree\").to_numpy()\n",
    "ticker_idx = returns[\"ticker\"].map(lambda x: ticker_2_int[x]).to_numpy()\n",
    "sect_idx = np.array([ticker_2_sect[x] for x in range(ntickers)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "m_hierarchical = pm.Model()\n",
    "\n",
    "with m_hierarchical:\n",
    "    # Data\n",
    "    _ri_m_rf = pm.Data(\"ri_m_rf\", ri_m_rf)\n",
    "    _rm_m_rf = pm.Data(\"rm_m_rf\", rm_m_rf)\n",
    "    _ticker_idx = pm.intX(pm.Data(\"ticker_idx\", ticker_idx))\n",
    "    _sect_idx = pm.intX(pm.Data(\"sect_idx\", sect_idx))\n",
    "\n",
    "    # Hyperprior\n",
    "    muhat = pm.Normal(\"mu_hat\", 0, 5, shape=nsect)\n",
    "    sigmahat = pm.HalfStudentT(\"sigma_hat\", 5, shape=nsect)\n",
    "\n",
    "    # Prior\n",
    "    beta_i = pm.Normal(\n",
    "        \"beta_i\", muhat[_sect_idx], sigmahat[_sect_idx],\n",
    "        shape=ntickers\n",
    "    )\n",
    "    sigma_i = pm.HalfStudentT(\"sigma_i\", 5, shape=ntickers)\n",
    "\n",
    "    # Likelihood\n",
    "    ll = pm.Normal(\"ll\", beta_i[_ticker_idx]*_rm_m_rf, sigma_i[_ticker_idx], observed=_ri_m_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "with m_hierarchical:\n",
    "    traces_hierarchical = pm.sample(2000, tune=1500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Diagnosing MCMC chains**\n",
    "\n",
    "There are a few diagnostic tools available:\n",
    "\n",
    "1. Rhat: This is a measure of how different the posterior samples across different chains are.\n",
    "  - You would like rhat to be very close to 1 -- If it's much higher than 1.01 or 1.02 then you should be worried about whether you're drawing samples from your posterior\n",
    "2. ESS: The effective sample size. If your draws are highly correlated (or have other problems) then the ESS becomes smaller\n",
    "  - It is possible to draw 1,000 values from your posterior but have less than 100 effective samples.\n",
    "  - Can compute the relative ESS which is $\\text{ESS} / n$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "with m_hierarchical:\n",
    "    ess = az.stats.diagnostics.ess(traces_hierarchical, relative=True)\n",
    "    rhat = az.stats.diagnostics.rhat(traces_hierarchical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "for _var in [\"beta_i\", \"sigma_i\", \"mu_hat\", \"sigma_hat\"]:\n",
    "    print(f\"Variable: {_var}\")\n",
    "    print(\"\\tMinimum relative ESS: \", ess[_var].min().values)\n",
    "    print(\"\\tMaximum rhat: \", rhat[_var].max().values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "traces_hierarchical.report._chain_warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Zoom's $\\beta$**\n",
    "\n",
    "Note that Zoom is an element of the \"Communication Services\" sector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "tick_sect.query(\"ticker == 'ZM'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Which is an identified by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "sect_2_int[\"Communication Services\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "with m_hierarchical:\n",
    "    az.plot_trace(traces_hierarchical, var_names=\"mu]_hat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with m_hierarchical:\n",
    "    az.plot_trace(traces_hierarchical, var_names=\"sigma_hat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "betas_nopool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "betas_hierarchical = pd.DataFrame(\n",
    "    {\n",
    "        \"beta_mean\": traces_hierarchical[\"beta_i\"].mean(axis=0),\n",
    "        \"beta_std\": traces_hierarchical[\"beta_i\"].std(axis=0),\n",
    "        \"sigma_mean\": traces_hierarchical[\"sigma_i\"].mean(axis=0),\n",
    "        \"sigma_std\": traces_hierarchical[\"sigma_i\"].std(axis=0)\n",
    "    }, index=tickers\n",
    ")\n",
    "\n",
    "betas_hierarchical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tick_sect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(14, 10))\n",
    "\n",
    "sector = \"Communication Services\"\n",
    "foo_tickers = tick_sect.query(f\"gics == '{sector}'\").loc[:, \"ticker\"].to_numpy()\n",
    "foo = betas_nopool.loc[foo_tickers, :]\n",
    "bar = betas_hierarchical.loc[foo_tickers, :]\n",
    "nfoo = foo.shape[0]\n",
    "\n",
    "ax.scatter(np.arange(nfoo), foo[\"beta_mean\"], color=\"r\", s=15)\n",
    "ax.scatter(np.arange(nfoo), bar[\"beta_mean\"], color=\"b\", s=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "We conclude with this excerpt about the benefits of multilevel (hierarchical) modeling from Statistical Rethinking by Richard McElreath:\n",
    "\n",
    "> 1. Improved estimates for repeat sampling. When more than one observation arises from the same individual, location, or time, then traditional, single-level models either maximally underfit or overfit the data.\n",
    "> 2. Improved estimates for imbalance in sampling. When some individuals, locations, or times are sampled more than others, multilevel models automatically cope with differing uncertainty across these clusters. This prevents over-sampled clusters from unfairly dominating inference.\n",
    "> 3. Estimates of variation. If our research questions include variation among individuals or other groups within the data, then multilevel models are a big help, because they model variation explicitly.\n",
    "> 4. Avoid averaging, retain variation. Frequently, scholars pre-average some data to construct variables. This can be dangerous, because averaging removes variation, and there are also typically several different ways to perform the averaging. Averaging therefore manufactures false confidence and introduces arbitrary data transformations. Multilevel models allow us to preserve the uncertainty and avoid data transformations"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
